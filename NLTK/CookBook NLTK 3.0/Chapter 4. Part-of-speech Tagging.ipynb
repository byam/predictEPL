{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this chapter, Bya will cover the following recipes:\n",
    "1. Default tagging\n",
    "2. Training a unigram part-of-speech tagger \n",
    "3. Combining taggers with backoff tagging \n",
    "4. Training and combining ngram taggers\n",
    "5. Creating a model of likely word tags\n",
    "6. Tagging with regular expressions\n",
    "7. Affix tagging\n",
    "8. Training a Brill tagger\n",
    "9. Training the TnT tagger\n",
    "10. Using WordNet for tagging\n",
    "11. Tagging proper names\n",
    "12. Classifier-based tagging\n",
    "13. Training a tagger with NLTK-Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part-of-speech** tagging is the process of converting a sentence, in the form of a list of words, into a list of tuples, where each tuple is of the form (**word, tag**). The **tag** is a part-of-speech tag, and signi es whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Default tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NN'), ('World', 'NN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "tagger.tag(['Hello', 'World'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14331966328512843"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello', 'NN'), ('world', 'NN'), ('.', 'NN')],\n",
       " [('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_sents([['Hello', 'world', '.'], ['How', 'are', 'you', '?']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untagging a tagged sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "\n",
    "untag([('Hello', 'NN'), ('World', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a unigram part-of-speech tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **unigram** generally refers to a single token. Therefore, a unigram tagger only uses a single word as its context for determining the part-of-speech tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train a  Unigram tagger with treebank corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Tokyo', ',', 'the', 'Nikkei', 'index', 'of', '225', 'selected', 'issues', ',', 'which', '*T*-1', 'gained', '132', 'points', 'Tuesday', ',', 'added', '14.99', 'points', 'to', '35564.43', '.'] \n",
      "\n",
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', None), ('index', 'NN'), ('of', 'IN'), ('225', 'CD'), ('selected', None), ('issues', 'NNS'), (',', ','), ('which', 'WDT'), ('*T*-1', '-NONE-'), ('gained', 'VBD'), ('132', None), ('points', 'NNS'), ('Tuesday', 'NNP'), (',', ','), ('added', 'VBD'), ('14.99', None), ('points', 'NNS'), ('to', 'TO'), ('35564.43', None), ('.', '.')] \n",
      "\n",
      "Evaluate: 0.8575868767537232\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "\n",
    "tagger = UnigramTagger(train_sents)\n",
    "\n",
    "print(treebank.sents()[3000], \"\\n\")\n",
    "print(tagger.tag(treebank.sents()[3000]), \"\\n\")\n",
    "\n",
    "print(\"Evaluate:\" ,tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8585365853658536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NN'),\n",
       " ('Vinken', None),\n",
       " (',', None),\n",
       " ('61', None),\n",
       " ('years', None),\n",
       " ('old', None),\n",
       " (',', None),\n",
       " ('will', None),\n",
       " ('join', None),\n",
       " ('the', None),\n",
       " ('board', None),\n",
       " ('as', None),\n",
       " ('a', None),\n",
       " ('nonexecutive', None),\n",
       " ('director', None),\n",
       " ('Nov.', None),\n",
       " ('29', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = UnigramTagger(model={'Pierre': 'NN'})\n",
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum frequency cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ContextTagger class uses frequency of occurrence to decide which tag is most likely for a given context. By default, it will do this even if the context word and tag occurs only once. If you'd like to set a minimum frequency threshold, then you can pass a cutoff value to the UnigramTagger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7756529246708397"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = UnigramTagger(train_sents, cutoff=3)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining tagger with backoff tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagger1 = DefaultTagger('NN')\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger2 = UnigramTagger(train_sents, backoff=tagger1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8742499460392834"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<UnigramTagger: size=8818>, <DefaultTagger: tag=NN>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger2._taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a trained tagger with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the tagger\n",
    "import pickle\n",
    "\n",
    "with open('tagger.pickle', 'wb') as f:\n",
    "    pickle.dump(tagger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the tagger\n",
    "import pickle\n",
    "\n",
    "with open('tagger.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training and combining ngram taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datas\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "test_sents = treebank.tagged_sents()[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', None), ('index', None), ('of', None), ('225', None), ('selected', None), ('issues', None), (',', None), ('which', None), ('*T*-1', None), ('gained', None), ('132', None), ('points', None), ('Tuesday', None), (',', None), ('added', None), ('14.99', None), ('points', None), ('to', None), ('35564.43', None), ('.', None)] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11305849341679257"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import BigramTagger\n",
    "bitagger = BigramTagger(train_sents)\n",
    "print(bitagger.tag(treebank.sents()[3000]), \"\\n\")\n",
    "bitagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', None), ('index', None), ('of', None), ('225', None), ('selected', None), ('issues', None), (',', None), ('which', None), ('*T*-1', None), ('gained', None), ('132', None), ('points', None), ('Tuesday', None), (',', None), ('added', None), ('14.99', None), ('points', None), ('to', None), ('35564.43', None), ('.', None)] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06850852579322253"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "tritagger = TrigramTagger(train_sents)\n",
    "print(tritagger.tag(treebank.sents()[3000]), \"\\n\")\n",
    "tritagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tag_util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    \n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tag_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6eab441f3f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtag_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackoff_tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDefaultTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnigramTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBigramTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrigramTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tag_util'"
     ]
    }
   ],
   "source": [
    "from tag_util import backoff_tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "backoff = DefaultTagger('NN')\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger,\n",
    "                                     TrigramTagger], backoff=backoff)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05836391107273905"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import NgramTagger\n",
    "\n",
    "quadtagger = NgramTagger(4, train_sents)\n",
    "quadtagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `taggers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import NgramTagger\n",
    "\n",
    "class QuadgramTagger(NgramTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        NgramTagger.__init__(self, 4, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8809842434707533"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from taggers import QuadgramTagger\n",
    "\n",
    "quadtagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger,\n",
    "                                          TrigramTagger, QuadgramTagger],\n",
    "                           backoff = backoff)\n",
    "quadtagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating a model of likely word tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tag_util.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can construct a model of the 200 most frequent words as keys, with the most frequent tag for each word as a value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def word_tag_model(words, tagged_words, limit=200):\n",
    "    fd = FreqDist(words)\n",
    "    cfd = ConditionalFreqDist(tagged_words)\n",
    "    \n",
    "    most_freq = (word for word, count in fd.most_common(limit))\n",
    "    \n",
    "    return dict((word, cfd[word].max()) for word in most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5594215411180661"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tag_util import word_tag_model\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "model = word_tag_model(treebank.words(), treebank.tagged_words())\n",
    "tagger = UnigramTagger(model=model)\n",
    "\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8790848262464925"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from tag_util import backoff_tagger\n",
    "\n",
    "default_tagger = DefaultTagger('NN')\n",
    "likely_tagger = UnigramTagger(model=model, backoff=default_tagger)\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                       backoff=likely_tagger)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8810274120440319"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger,\n",
    "TrigramTagger], backoff=default_tagger)\n",
    "\n",
    "likely_tagger = UnigramTagger(model=model, backoff=tagger)\n",
    "likely_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Tagging with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tag_util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns = [\n",
    "     (r'^\\d+$', 'CD'), # cardinal numbers i.e 1 2 3\n",
    "     (r'.*ing$', 'VBG'), # gerunds, i.e. wondering\n",
    "     (r'.*ment$', 'NN'), # i.e. wonderment\n",
    "     (r'.*ful$', 'JJ') # i.e. wonderful\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037470321605870924"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tag_util import patterns\n",
    "from nltk.tag import RegexpTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "tagger = RegexpTagger(patterns)\n",
    "\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default arguments for an **AffixTagger** class specify three-character suffixes, and that words must be at least five characters long. If a word is less than five characters, then None is returned as the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "test_sents = treebank.tagged_sents()[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Affix tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', None), ('Tokyo', 'NNP'), (',', None), ('the', None), ('Nikkei', None), ('index', 'NN'), ('of', None), ('225', None), ('selected', 'VBN'), ('issues', 'NNS'), (',', None), ('which', 'WDT'), ('*T*-1', '-NONE-'), ('gained', 'VBN'), ('132', None), ('points', 'NNS'), ('Tuesday', 'NN'), (',', None), ('added', 'VBD'), ('14.99', None), ('points', 'NNS'), ('to', None), ('35564.43', None), ('.', None)] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27502698035829914"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import AffixTagger\n",
    "\n",
    "tagger = AffixTagger(train_sents)\n",
    "print(tagger.tag(treebank.sents()[3000]), \"\\n\")\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23621843298078998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_tagger = AffixTagger(train_sents, affix_length=3)\n",
    "prefix_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3004101014461472"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_tagger = AffixTagger(train_sents, affix_length=2)\n",
    "suffix_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training a TnT tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8756313403842003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import tnt\n",
    "tnt_tagger = tnt.TnT()\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8925102525361537"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "unk = DefaultTagger('NN')\n",
    "tnt_tagger = tnt.TnT(unk=unk, Trained=True)\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8756313403842003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnt_tagger = tnt.TnT(N=100)\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Using WordNet for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "test_sents = treebank.tagged_sents()[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| WordNet tag        | Treebank tag|\n",
    "|-------------:| -----:|\n",
    "| n      | NN | \n",
    "| a      | JJ      |   \n",
    "| s | JJ      |\n",
    "| r|RB |\n",
    "| v|VB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `taggers.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a class that will look up words in **WordNet**, and then choose the most common tag from the **Synsets** it finds. The **WordNetTagger** class defined in the following code can be found in **taggers.py**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Now we can create a class that will look up words in WordNet,\n",
    "# and then choose the most common tag from the Synsets it finds.\n",
    "class WordNetTagger(SequentialBackoffTagger):\n",
    "    '''\n",
    "    >>> wt = WordNetTagger()\n",
    "    >>> wt.tag(['food', 'is', 'great'])\n",
    "    [('food', 'NN'), ('is', 'VB'), ('great', 'JJ')]\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "\n",
    "        self.wordnet_tag_map = {\n",
    "            'n': 'NN',\n",
    "            's': 'JJ',\n",
    "            'a': 'JJ',\n",
    "            'r': 'RB',\n",
    "            'v': 'VB'\n",
    "        }\n",
    "\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        fd = FreqDist()\n",
    "\n",
    "        for synset in wordnet.synsets(word):\n",
    "            fd[synset.pos()] += 1\n",
    "\n",
    "        if not fd:\n",
    "            return None\n",
    "        return self.wordnet_tag_map.get(fd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1858445898001574"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from taggers import WordNetTagger\n",
    "wn_tagger = WordNetTagger()\n",
    "wn_tagger.evaluate(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846967407727174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tag_util import backoff_tagger\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger,\n",
    "TrigramTagger], backoff=wn_tagger)\n",
    "\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Tagging proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.corpus import names\n",
    "\n",
    "class NamesTagger(SequentialBackoffTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        self.name_set = set([n.lower() for n in names.words()])\n",
    "\n",
    "        def choose_tag(self, tokens, index, history):\n",
    "            word = tokens[index]\n",
    "            if word.lower() in self.name_set:\n",
    "                return 'NNP'\n",
    "            else:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jacob', 'NNP')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from taggers import NamesTagger\n",
    "\n",
    "nt = NamesTagger()\n",
    "nt.tag(['Jacob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Classifier-based tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9309734513274336"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "tagger = ClassifierBasedPOSTagger(train=train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. NLTK-Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train entire treebank corpus\n",
    "\n",
    "The default training algorithm is **aubt**, which is shorthand for a sequential backoff tagger composed of \n",
    "\n",
    "`AffixTagger + UnigramTagger + BigramTagger + TrigramTagger`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank\n",
    "loading treebank\n",
    "3914 tagged sents, training on 3914\n",
    "training AffixTagger with affix -3 and backoff <DefaultTagger: tag=-None->\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <AffixTagger: size=2536>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=4940>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=2328>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.992362\n",
    "dumping TrigramTagger to /Users/Bya/nltk_data/taggers/treebank_aubt.pickle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train_tagger.py script roughly performers the following steps:\n",
    "1. Construct training and testing sentences from corpus arguments. \n",
    "2. Build tagger training function from tagger arguments.\n",
    "3. Train a tagger on the training sentences using the training function. \n",
    "4. Evaluate and/or save the tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python train_tagger.py treebank --fraction 0.75 --no-pickle\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training AffixTagger with affix -3 and backoff <DefaultTagger: tag=-None->\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <AffixTagger: size=2287>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=4176>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=1836>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.906082\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Tagger\n",
    "\n",
    "Using **--default NN** lets us assign a default tag of **NN**, while **--sequential ''** disables the default **aubt** sequential backoff algorithm. The **--fraction** argument is omitted in this case because there's not actually any training happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --default NN --sequential ''\n",
    "loading treebank\n",
    "3914 tagged sents, training on 3914\n",
    "evaluating DefaultTagger\n",
    "accuracy: 0.130776\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying **--sequential u** tells **train_tagger.py** to train with a **unigram tagger**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --fraction 0.75 --sequential u\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <DefaultTagger: tag=-None->\n",
    "evaluating UnigramTagger\n",
    "accuracy: 0.856327\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did earlier, we can boost the accuracy a bit by using a default tagger:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --default NN --fraction 0.75 --sequential u\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <DefaultTagger: tag=NN>\n",
    "evaluating UnigramTagger\n",
    "accuracy: 0.874387\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Bigram Tagger and Trigram Tagger:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --default NN --fraction 0.75 --sequential ubt\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <DefaultTagger: tag=NN>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=8709>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=1836>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.879213\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affix\n",
    "The default training algorithm is **--sequential aubt**, and the default affix is **-3**. But you can modify this with one or more **-a** arguments. So, if we want to use an affix of **-2** as well as an affix of **-3**, you can do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --default NN --fraction 0.75 -a -3 -a -2\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training AffixTagger with affix -3 and backoff <DefaultTagger: tag=NN>\n",
    "training AffixTagger with affix -2 and backoff <AffixTagger: size=2143>\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <AffixTagger: size=248>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=5207>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=1836>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.907328\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of multiple **-a** arguments matters, and if you switch the order, the results and accuracy will change, because the backoff order changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --default NN --fraction 0.75 -a -2 -a -3\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training AffixTagger with affix -2 and backoff <DefaultTagger: tag=NN>\n",
    "training AffixTagger with affix -3 and backoff <AffixTagger: size=606>\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <AffixTagger: size=1318>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=4176>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=1836>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.914166\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-Based Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can train a **classifier-based** tagger with the **--classifier argument**, which specifies the name of a classifier. Be sure to also pass in **--sequential ''** because, as we learned previously, training a sequential backoff tagger in addition to a classifier-based tagger is useless. The **--default** argument is also useless, because the classifier will always guess something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --fraction 0.75 --sequential '' --classifier NaiveBayes\n",
    "loading treebank\n",
    "3914 tagged sents, training on 2936\n",
    "training ['NaiveBayes'] ClassifierBasedPOSTagger\n",
    "Constructing training corpus for classifier.\n",
    "Training classifier (75814 instances)\n",
    "training NaiveBayes classifier\n",
    "evaluating ClassifierBasedPOSTagger\n",
    "accuracy: 0.928686\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **classifier-based** taggers tend to be **more accurate**, they are also slower to train, and much slower at tagging. If speed is important to you, I recommend sticking with sequential taggers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a pickled tagger\n",
    "\n",
    "Without the **--no-pickle** argument, train_tagger.py will save a pickled tagger at **~/nltk_data/taggers/NAME.pickle**, where **NAME** is a combination of the corpus name and training algorithm. You can specify a custom filename for your tagger using the **--filename** argument like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --filename path/to/tagger.pickle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on a custom corpus\n",
    "\n",
    "If you have a **custom corpus** that you want to use for training a tagger, you can do that by passing in the path to the corpus and the classname of a corpus reader in the **--reader** argument. The corpus path can either be absolute or relative to a **nltk_data** directory. The corpus reader class must provide a **tagged_sents()** method. Here's an example using a relative path to the treebank tagged corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py corpora/treebank/tagged --reader nltk.corpus.reader.ChunkedCorpusReader --no-pickle --fraction 0.75\n",
    "loading corpora/treebank/tagged\n",
    "51002 tagged sents, training on 38252\n",
    "training AffixTagger with affix -3 and backoff <DefaultTagger: tag=-None->\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <AffixTagger: size=2092>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=4125>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=1626>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.882123\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with universal tags\n",
    "\n",
    "Because the universal tagset has fewer tags, these taggers tend to be more accurate;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python train_tagger.py treebank --no-pickle --fraction 0.75 --tagset universal\n",
    "loading treebank\n",
    "using universal tagset\n",
    "3914 tagged sents, training on 2936\n",
    "training AffixTagger with affix -3 and backoff <DefaultTagger: tag=-None->\n",
    "training <class 'nltk.tag.sequential.UnigramTagger'> tagger with backoff <AffixTagger: size=2287>\n",
    "training <class 'nltk.tag.sequential.BigramTagger'> tagger with backoff <UnigramTagger: size=2884>\n",
    "training <class 'nltk.tag.sequential.TrigramTagger'> tagger with backoff <BigramTagger: size=1023>\n",
    "evaluating TrigramTagger\n",
    "accuracy: 0.934116\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing a tagger against a tagged corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('out', 'RP'),\n",
       " ('.', '.'),\n",
       " ('Just', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('me', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "text=nltk.word_tokenize(\"We are going out. Just you and me.\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the tagger\n",
    "import pickle\n",
    "\n",
    "with open('/Users/Bya/nltk_data/taggers/treebank_NaiveBayes.pickle', 'rb') as f:\n",
    "    tagger_nb = pickle.load(f)\n",
    "\n",
    "with open('/Users/Bya/nltk_data/taggers/treebank_aubt.pickle', 'rb') as f:\n",
    "    tagger_aubt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Come', 'DT'), ('on', 'IN'), ('gunners', 'NNS'), ('!', '.'), ('#NUFCvAFC', 'NN'), ('#Arsenal', 'JJ')]\n",
      "[('Come', 'DT'), ('on', 'IN'), ('gunners', 'NNS'), ('!', '.'), ('#NUFCvAFC', '-RRB-'), ('#Arsenal', 'JJ')]\n",
      "\n",
      "\n",
      "[('come', 'VB'), ('gunner', 'NN'), ('!', '.'), ('nufcvafc', 'NN'), ('arsenal', 'JJ')]\n",
      "[('come', 'DT'), ('gunner', 'JJR'), ('!', '.'), ('nufcvafc', 'PDT'), ('arsenal', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "sample1 = ['Come', 'on', 'gunners', '!', '#NUFCvAFC', '#Arsenal'] \n",
    "sample2 = ['come', 'gunner', '!', 'nufcvafc', 'arsenal'] \n",
    "print(tagger_aubt.tag(sample1))\n",
    "print(tagger_nb.tag(sample1))\n",
    "print(\"\\n\")\n",
    "print(tagger_aubt.tag(sample2))\n",
    "print(tagger_nb.tag(sample2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
