{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this chapter, Bya will cover the following recipes:\n",
    "1. Tokenizing text into sentences\n",
    "2. Tokenizing sentences into words\n",
    "3. Tokenizing sentences using regular expressions \n",
    "4. Training a sentence tokenizer\n",
    "5. Filtering stopwords in a tokenized sentence\n",
    "6. Looking up Synsets for a word in WordNet\n",
    "7. Looking up lemmas and synonyms in WordNet \n",
    "8. Calculating WordNet Synset similarity\n",
    "9. Discovering word collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizing text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", \"This is the Mr.Bya's seminar code.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph\n",
    "para = \"Hello World. It's good to see you. This is the Mr.Bya's seminar code.\"\n",
    "\n",
    "# import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# tokenize paragraph\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  If tokenizing a lot of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  it's more efficient to load the PunktSentenceTokenizer class once, \n",
    "# and call its tokenize() method instead:\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('/Users/Bya/nltk_data/tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sentences in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating words using spaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', 'ca', \"n't\", 'this', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Hello World can't this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World.', 'Mr.Bya', 'ca', \"n't\", 'do', 'this', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Hello World. Mr.Bya can't do \\t this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(\"can't\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', '.', 'Bya', 'can', \"'\", 't', 'do', 'this', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all punctuation into separate tokens\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Hello World. \\n Bya can't do \\t this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World.', 'Bya', \"can't\", 'do', 'this.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all punctuation into separate tokens\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenizer.tokenize(\"Hello World. \\n Bya can't do \\t this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenizing sentences using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', 'Bya', \"can't\", 'do', 'this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Hello World. \\n Bya can't do \\t this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple usage of RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as above\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple whitespace tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The gaps=True parameter means that the pattern is used to identify gaps to tokenize on. If we used gaps=False, then the pattern would be used to identify tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World.', 'Bya', \"can't\", 'do', 'this.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"Hello World. \\n Bya can't do \\t this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', \"n't is \", ' contr', 'ction.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"a\", gaps=True)\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ￼￼Training a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents1[678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all theatrical.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents2[678]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/Bya/nltk_data/corpora/webtext/overheard.txt', encoding='ISO-8859-2') as f:\n",
    "    text = f.read()\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "sents = sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[678]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filtering stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Stopwords are common words that generally do not contribute to the meaning of a sentence, at least for the purposes of information retrieval and natural language processing. These are words such as the and a. Most search engines will filter out stopwords from search queries and documents in order to save space in their index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import english stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# defining stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# filtering stopwords\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add word to Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import english stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# defining stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# add word to stopwords\n",
    "english_stops_added = english_stops | {'contraction'}\n",
    "\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "[word for word in words if word not in english_stops_added]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Looking up Synsets for a word in WordNet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WordNet is a lexical database for the English language. In other words, it's a dictionary designed specifically for natural language processing.\n",
    "\n",
    "NLTK comes with a simple interface to look up words in WordNet. What you get is a list of Synset instances, which are groupings of synonymous words that express the same concept. Many words have only one Synset, but some have several. In this recipe, we'll explore a single Synset, and in the next recipe, we'll look at several in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: cookbook.n.01 \n",
      "\n",
      "Definition: a book of recipes and cooking directions \n",
      "\n",
      "Examples: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing Wordnet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# look up the Synset for 'cookbook'\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "\n",
    "print(\"Name: %s \\n\" % syn.name())\n",
    "print(\"Definition: %s \\n\" % syn.definition())\n",
    "print(\"Examples: %s \\n\" % syn.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with hypernyms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Synsets are organized in a structure similar to that of an inheritance tree. More abstract terms are known as hypernyms and more specific terms are hyponyms. This tree can be traced all the way up to a root hypernym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms of Synset('cookbook.n.01'): \n",
      "\n",
      " \t [Synset('reference_book.n.01')]\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('cookbook')[0]\n",
    "\n",
    "print(\"Hypernyms of %s: \\n\\n \\t %s\" % (syn, syn.hypernyms()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyponyms of Synset('reference_book.n.01'):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('annual.n.02'),\n",
       " Synset('atlas.n.02'),\n",
       " Synset('cookbook.n.01'),\n",
       " Synset('directory.n.01'),\n",
       " Synset('encyclopedia.n.01'),\n",
       " Synset('handbook.n.01'),\n",
       " Synset('instruction_book.n.01'),\n",
       " Synset('source_book.n.01'),\n",
       " Synset('wordbook.n.01')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Hyponyms of %s:\" % (syn.hypernyms()[0]))\n",
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root of 'cookbook'\n",
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('creation.n.02'),\n",
       "  Synset('product.n.02'),\n",
       "  Synset('work.n.02'),\n",
       "  Synset('publication.n.01'),\n",
       "  Synset('book.n.01'),\n",
       "  Synset('reference_book.n.01'),\n",
       "  Synset('cookbook.n.01')]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire path of 'cookbook'\n",
    "# hypernym_paths() method returns a list of lists\n",
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Of Speech (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In WordNet 4 types of POS\n",
    "* n - Noun\n",
    "* a - Adjective\n",
    "* r - Adverb\n",
    "* v - Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('cookbook')[0]\n",
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(wordnet.synsets('great')))\n",
    "print(len(wordnet.synsets('great', pos='n')))\n",
    "print(len(wordnet.synsets('great', pos='a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Looking up Lemmas and Synonyms in WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cookbook', 'cookery_book']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "\n",
    "[lemma.name() for lemma in syn.lemmas()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All possible synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('book'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "print(len(synonyms))\n",
    "print(len(set(synonyms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moral excellence or admirableness\n",
      "<bound method Lemma.name of Lemma('evil.n.03.evil')>\n",
      "the quality of being morally wrong in principle or practice\n",
      "having desirable or positive qualities especially those suitable for a thing specified\n",
      "bad\n",
      "having undesirable or negative qualities\n"
     ]
    }
   ],
   "source": [
    "gn2 = wordnet.synset('good.n.02')\n",
    "print(gn2.definition())\n",
    "\n",
    "evil = gn2.lemmas()[0].antonyms()[0]\n",
    "print(evil.name)\n",
    "\n",
    "print(evil.synset().definition())\n",
    "\n",
    "ga1 = wordnet.synset('good.a.01')\n",
    "print(ga1.definition())\n",
    "\n",
    "bad = ga1.lemmas()[0].antonyms()[0]\n",
    "print(bad.name())\n",
    "\n",
    "print(bad.synset().definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Calculating WordNet Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "\n",
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing Verbs\n",
    "cook = wordnet.synset('cook.v.01')\n",
    "bake = wordnet.synset('bake.v.02')\n",
    "cook.wup_similarity(bake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Discovering Word Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble'),\n",
       " ('squeak', 'squeak'),\n",
       " ('saw', 'saw'),\n",
       " ('holy', 'grail'),\n",
       " ('run', 'away'),\n",
       " ('french', 'guard'),\n",
       " ('cartoon', 'character')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TrigramCollocationFinder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 'term', 'relationship')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "tcf.apply_freq_filter(3)\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
