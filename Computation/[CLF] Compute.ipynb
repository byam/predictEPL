{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global Imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Scikit-Learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Local Imports\n",
    "path = str(os.path.expanduser('~')) + '/git/predictEPL/config'\n",
    "sys.path.append(path)\n",
    "import paths\n",
    "\n",
    "sys.path.append(paths.UTILS)\n",
    "import tokenizer\n",
    "import useful_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Support Vector Machine\n",
    "# return: gridsearch SVM\n",
    "def SVM(y_train, n_folds=10):\n",
    "    # putting the steps explicitly into Pipeline\n",
    "    pipeline_svm = Pipeline([\n",
    "            # strings to token counts to weighted TF-IDF scores\n",
    "            ('vect', TfidfVectorizer(\n",
    "                    analyzer=tokenizer.Lemma,  # extract the sequence of features out of the raw\n",
    "                    use_idf=True,              # Enable inverse-document-frequency reweighting\n",
    "                    max_df=1.0,                # frequency threshold\n",
    "                    max_features=None,         # max features\n",
    "                    )),\n",
    "\n",
    "            # train on vectors with classifier\n",
    "            ('clf', SVC(\n",
    "                    kernel='linear',\n",
    "                    probability=True\n",
    "                ))\n",
    "        ])\n",
    "\n",
    "    # tunning parameters\n",
    "    params_svm = {\n",
    "        'vect__analyzer': (\n",
    "            tokenizer.Lemma, tokenizer.LemmaNoSoccerStops\n",
    "        ),\n",
    "#         'clf__kernel': ('linear', 'rbf')\n",
    "#         'clf__gamma': (0.00001, 0.0001, 00.1),\n",
    "        # 'clf__C': (1, 10, 100),\n",
    "    }\n",
    "\n",
    "    # grid search\n",
    "    grid_svm = GridSearchCV(\n",
    "        pipeline_svm,        # pipeline from above\n",
    "        params_svm,          # parameters to tune via cross validation\n",
    "        refit=True,          # fit using all available data at the end, on the best found param combination\n",
    "        n_jobs=-1,           # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "        scoring='accuracy',  # what score are we optimizing?\n",
    "        cv=StratifiedKFold(y_train, n_folds=n_folds),  # what type of cross validation to use\n",
    "    )\n",
    "\n",
    "    return grid_svm\n",
    "\n",
    "\n",
    "# Print Training Parameters\n",
    "def DetecterParams(detecter, title=\"\", all_tunes=True):\n",
    "    print(\"\\n\\n### PARAMS ################################\\n\")\n",
    "\n",
    "    if all_tunes:\n",
    "        print(\"[All Params Results]:\\n\")\n",
    "        pprint(detecter.grid_scores_)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"[%s Detecter Params]: \\n\" % title)\n",
    "    print(\"Best Score: \", detecter.best_score_)\n",
    "    print(\"Best Params: \", detecter.best_params_)\n",
    "\n",
    "\n",
    "# Print Test Prediction\n",
    "def DetecterMetrics(features, labels, detecter, title=\"\"):\n",
    "    predictions = detecter.predict(features)\n",
    "    print(\"\\n\\n### METRICS ###############################\\n\")\n",
    "\n",
    "    print(\"[%s Results]: \\n\" % title)\n",
    "    print(classification_report(labels, predictions))\n",
    "    print('[Accuracy]: ', accuracy_score(labels, predictions))\n",
    "\n",
    "\n",
    "# Classifier Train Process\n",
    "def ClassifierTrain(save=False):\n",
    "    date_now = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()).replace(\" \", \"_\")\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 1]: Data Load\n",
    "    # ***************************************************\n",
    "\n",
    "    # Read Hashtags in Emotion Words Tweets\n",
    "#     df = useful_methods.csv_dic_df(paths.DATA_HOME + \"TweetsPN/tweet_hash_emolex_pn.csv\")\n",
    "    \n",
    "    # Rotten Tomatos\n",
    "    df = useful_methods.csv_dic_df(paths.DATA_HOME + \"TweetsPN/short_movie_reviews.csv\")\n",
    "\n",
    "\n",
    "    # positive: 1, negative: 0\n",
    "    df['label'] = [int(label) for label in df['label']]\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 2]: Data Split(train=0.8, test=0.2)\n",
    "    # ***************************************************\n",
    "\n",
    "    # Split data Train and Test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'],\n",
    "        df['label'],\n",
    "        test_size=0.2\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"\\n\\n### DATA ##################################\\n\",\n",
    "        \"\\n\\tTrain data: \\t\", len(X_train),\n",
    "        \"\\n\\tTest data: \\t\", len(X_test),\n",
    "        \"\\n\\tAll data: \\t\", len(y_train) + len(y_test)\n",
    "    )\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 3]: Define Classifier\n",
    "    # ***************************************************\n",
    "\n",
    "    grid_search = SVM(y_train)\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 4]: Compute Classifier\n",
    "    # ***************************************************\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    # fitting training sets to classifier\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 4]: Print Classifier Details\n",
    "    # ***************************************************\n",
    "\n",
    "    # print trained parameters\n",
    "    DetecterParams(grid_search, title=\"SVM\")\n",
    "\n",
    "    # print computed time\n",
    "    print(\"\\n\\n### COMPUTED TIME #########################\\n\")\n",
    "    taken_time = time() - start_time\n",
    "    print(\"[Started Time]: \", date_now)\n",
    "    print(\"\\n[Taken Time]: \", str(datetime.timedelta(seconds=taken_time)))\n",
    "\n",
    "    # print classifier test results\n",
    "    DetecterMetrics(X_test, y_test, grid_search, title=\"Test\")\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 5]: Save Classifier Details\n",
    "    # ***************************************************\n",
    "\n",
    "    if save:\n",
    "        filename = \"dtr_hash_svn_\" + date_now + \".pkl\"\n",
    "        with open(paths.DETECTER_HOME + filename, 'wb') as fout:\n",
    "            pickle.dump(grid_search, fout)\n",
    "            print(\"\\n\\n[Saved in]: \", paths.DETECTER_HOME + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### DATA ##################################\n",
      " \n",
      "\tTrain data: \t 8529 \n",
      "\tTest data: \t 2133 \n",
      "\tAll data: \t 10662\n",
      "\n",
      "\n",
      "### PARAMS ################################\n",
      "\n",
      "[All Params Results]:\n",
      "\n",
      "[mean: 0.75976, std: 0.01175, params: {'vect__analyzer': <function Lemma at 0x1129d2730>},\n",
      " mean: 0.74851, std: 0.01229, params: {'vect__analyzer': <function LemmaNoSoccerStops at 0x1129d28c8>}]\n",
      "\n",
      "\n",
      "[SVM Detecter Params]: \n",
      "\n",
      "Best Score:  0.759760816039\n",
      "Best Params:  {'vect__analyzer': <function Lemma at 0x1129d2730>}\n",
      "\n",
      "\n",
      "### COMPUTED TIME #########################\n",
      "\n",
      "[Started Time]:  2016-01-24_05:14:42\n",
      "\n",
      "[Taken Time]:  0:08:00.591201\n",
      "\n",
      "\n",
      "### METRICS ###############################\n",
      "\n",
      "[Test Results]: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.77      0.77      1088\n",
      "          1       0.76      0.77      0.76      1045\n",
      "\n",
      "avg / total       0.77      0.77      0.77      2133\n",
      "\n",
      "[Accuracy]:  0.766994842944\n",
      "\n",
      "\n",
      "[Saved in]:  /Users/Bya/Dropbox/Research/datas/Detecter/dtr_hash_svn_2016-01-24_05:14:42.pkl\n"
     ]
    }
   ],
   "source": [
    "ClassifierTrain(save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
