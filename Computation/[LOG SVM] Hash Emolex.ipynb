{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Global Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import datetime\n",
    "from time import gmtime, strftime\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "# Scikit-Learn imports\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Local Imports\n",
    "path = str(os.path.expanduser('~')) + '/git/predictEPL/config'\n",
    "sys.path.append(path)\n",
    "import paths\n",
    "\n",
    "sys.path.append(paths.UTILS)\n",
    "import useful_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GW', 'away_team', 'date', 'home_team', 'pn_away_neg', 'pn_away_pos',\n",
       "       'pn_home_neg', 'pn_home_pos', 'score_ft_away', 'score_ft_home',\n",
       "       'score_ht_away', 'score_ht_home', 'time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = useful_methods.csv_dic_df(paths.DATA_HOME + \"EPL/all_game_hash_emolex_pn_score.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Data: all_game_emolex_counted\n",
    "def ReadEmoleDf():\n",
    "    df = useful_methods.csv_dic_df(paths.DATA_HOME + \"EPL/all_game_hash_emolex_pn_score.csv\")\n",
    "#     df = useful_methods.csv_dic_df(paths.DATA_HOME + \"EPL/all_game_hash_review_pn_score.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    # Manipulations\n",
    "\n",
    "    # 'home_win': 1, 'away_win': 0, 'draw': 2\n",
    "    def Labeling(goal_diff):\n",
    "        if goal_diff > 0:\n",
    "            return 1\n",
    "        elif goal_diff < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    df.score_ft_home = [int(score_ft_home) for score_ft_home in df.score_ft_home]\n",
    "    df.score_ft_away = [int(score_ft_away) for score_ft_away in df.score_ft_away]\n",
    "    \n",
    "    df.score_ht_home = [int(score_ht_home) for score_ht_home in df.score_ht_home]\n",
    "    df.score_ht_away = [int(score_ht_away) for score_ht_away in df.score_ht_away]\n",
    "\n",
    "    df['goal_diff_ht'] = df.score_ht_home - df.score_ht_away\n",
    "    df['goal_diff'] = df.score_ft_home - df.score_ft_away\n",
    "    df['result'] = [Labeling(goal_diff) for goal_diff in df.goal_diff]\n",
    "    \n",
    "    df.pn_home_pos = [float(pn_home_pos) for pn_home_pos in df.pn_home_pos]\n",
    "    df.pn_home_neg = [float(pn_home_neg) for pn_home_neg in df.pn_home_neg]\n",
    "    df.pn_away_pos = [float(pn_away_pos) for pn_away_pos in df.pn_away_pos]\n",
    "    df.pn_away_neg = [float(pn_away_neg) for pn_away_neg in df.pn_away_neg]\n",
    "    \n",
    "    df['pn_diff_pos'] = df.pn_home_pos - df.pn_away_pos\n",
    "    \n",
    "    \n",
    "    # **********************************************\n",
    "    dfBe4GameSum = useful_methods.csv_dic_df(paths.DATA_HOME + \"EPL/info_before_4game_sum.csv\")\n",
    "\n",
    "    team_homes = list(df.home_team)\n",
    "    team_aways = list(df.away_team)\n",
    "    GWs = [int(gw) for gw in df.GW]\n",
    "\n",
    "    be_4game_sum_home = []\n",
    "    be_4game_sum_away = []\n",
    "\n",
    "    for index in range(len(GWs)):\n",
    "        team_home = team_homes[index]\n",
    "        team_away = team_aways[index]\n",
    "        gw = GWs[index]\n",
    "\n",
    "        be_4game_sum_home.append(int(dfBe4GameSum[dfBe4GameSum.team == team_home][str(gw)]))\n",
    "        be_4game_sum_away.append(int(dfBe4GameSum[dfBe4GameSum.team == team_away][str(gw)]))\n",
    "\n",
    "    df['be_4game_sum_home'] = be_4game_sum_home\n",
    "    df['be_4game_sum_away'] = be_4game_sum_away\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df for models.\n",
    "def CreateDfModel(draw=False, ht_draw=False):\n",
    "    df = ReadEmoleDf()\n",
    "    \n",
    "    # delete NA data\n",
    "    df = df[df.pn_home_pos != -1.0].copy().reset_index(drop=True)\n",
    "    \n",
    "    # only for Win or Lose\n",
    "    if not draw:\n",
    "        df = df[df.result != 2].copy().reset_index(drop=True)\n",
    "    \n",
    "    # HT: Equal\n",
    "    if ht_draw:\n",
    "        df = df[df.goal_diff_ht == 0].copy().reset_index(drop=True)\n",
    "    \n",
    "    dta = pd.DataFrame()\n",
    "    \n",
    "    # Teams\n",
    "    dta['team_home'] = df.home_team\n",
    "    dta['team_away'] = df.away_team\n",
    "\n",
    "    # POS, NEG\n",
    "    dta['pn_home_pos'] = df.pn_home_pos / (df.pn_home_pos + df.pn_home_neg)\n",
    "    dta['pn_home_neg'] = df.pn_home_neg / (df.pn_home_pos + df.pn_home_neg)\n",
    "    dta['pn_away_pos'] = df.pn_away_pos / (df.pn_away_pos + df.pn_away_neg)\n",
    "    dta['pn_away_neg'] = df.pn_away_neg / (df.pn_away_pos + df.pn_away_neg)\n",
    "    \n",
    "    dta['pn_diff_pos'] = dta['pn_home_pos'] - dta['pn_away_pos']\n",
    "    \n",
    "    # HF scores\n",
    "    dta['score_ht_home'] = df.score_ht_home\n",
    "    dta['score_ht_away'] = df.score_ht_away\n",
    "    \n",
    "    dta['goal_diff_ht'] = df['goal_diff_ht']\n",
    "\n",
    "    # 'home_win': 1, 'away_win': 0, 'draw': 2\n",
    "    dta['result'] = df.result\n",
    "    \n",
    "    # be_4game_sum\n",
    "    dta['be_4game_sum_home'] = df.be_4game_sum_home / 12.0\n",
    "    dta['be_4game_sum_away'] = df.be_4game_sum_away / 12.0\n",
    "    \n",
    "    return dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X: df, y: list\n",
    "def CreateXy(df, team_name=False, hash_emolex=True, score_ht=True, be_4game_sum=False):\n",
    "    if team_name and hash_emolex and score_ht and be_4game_sum:\n",
    "        y, X = dmatrices('result ~ \\\n",
    "            be_4game_sum_home + be_4game_sum_away + \\\n",
    "            score_ht_home + score_ht_away + \\\n",
    "            pn_home_pos + pn_home_neg + pn_away_pos + pn_away_neg + \\\n",
    "            pn_diff_pos + \\\n",
    "            C(team_home) + C(team_away)',\n",
    "            df, return_type=\"dataframe\")\n",
    "    elif hash_emolex and score_ht and be_4game_sum:\n",
    "        y, X = dmatrices('result ~ \\\n",
    "            be_4game_sum_home + be_4game_sum_away + \\\n",
    "            score_ht_home + score_ht_away + \\\n",
    "            pn_home_pos + pn_home_neg + pn_away_pos + pn_away_neg + \\\n",
    "            pn_diff_pos',\n",
    "            df, return_type=\"dataframe\")\n",
    "    elif hash_emolex and be_4game_sum:\n",
    "        y, X = dmatrices('result ~ \\\n",
    "            be_4game_sum_home + be_4game_sum_away + \\\n",
    "            pn_home_pos + pn_home_neg + pn_away_pos + pn_away_neg + \\\n",
    "            pn_diff_pos',\n",
    "            df, return_type=\"dataframe\")\n",
    "    elif hash_emolex:\n",
    "        y, X = dmatrices('result ~ \\\n",
    "            pn_home_pos + pn_home_neg + pn_away_pos + pn_away_neg + \\\n",
    "            pn_diff_pos',\n",
    "            df, return_type=\"dataframe\")\n",
    "    else:\n",
    "        y, X = dmatrices('result ~ \\\n",
    "            score_ht_home + score_ht_away',\n",
    "            df, return_type=\"dataframe\")\n",
    "        \n",
    "    # flatten y into a 1-D array\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print Training Parameters\n",
    "def DetecterParams(detecter, title=\"\", all_tunes=True):\n",
    "    print(\"\\n\\n### PARAMS ################################\\n\")\n",
    "\n",
    "    if all_tunes:\n",
    "        print(\"[All Params Results]:\\n\")\n",
    "        pprint(detecter.grid_scores_)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"[%s Detecter Params]: \\n\" % title)\n",
    "    print(\"Best Score: \", detecter.best_score_)\n",
    "    print(\"Best Params: \", detecter.best_params_)\n",
    "\n",
    "\n",
    "# Print Test Prediction\n",
    "def DetecterMetrics(features, labels, detecter, title=\"\"):\n",
    "    predictions = detecter.predict(features)\n",
    "    print(\"\\n\\n### METRICS ###############################\\n\")\n",
    "\n",
    "    print(\"[%s Results]: \\n\" % title)\n",
    "    print(metrics.classification_report(labels, predictions))\n",
    "    print('[Accuracy]: ', metrics.accuracy_score(labels, predictions))\n",
    "\n",
    "\n",
    "# Receiver Operating Characteristic = ROC curve\n",
    "# Visualizes a classifier's performance\n",
    "# for all values of the discrimination threshold. \n",
    "# fall out: F = FP / (TN + FP)\n",
    "# AUC (area under the curve)\n",
    "def PlotRocAuc(features, labels, detecter, title=\"\"):\n",
    "    # predict features\n",
    "    predictions = detecter.predict_proba(features)\n",
    "    \n",
    "    # calculate Fall Out & Recall\n",
    "    false_positive_rate, recall, thresholds = metrics.roc_curve(\n",
    "        labels, predictions[:, 1])\n",
    "\n",
    "    # ROC AUC\n",
    "    roc_auc = metrics.auc(false_positive_rate, recall)\n",
    "\n",
    "    # Plot\n",
    "    plt.title('Receiver Operating Characteristic: ' + title)\n",
    "    plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out')\n",
    "    plt.show()\n",
    "\n",
    "# ****************************************************************************\n",
    "# ****************************************************************************\n",
    "\n",
    "# Define Logistic Recression\n",
    "def Log(y_train, n_folds=10):\n",
    "    # putting the steps explicitly into Pipeline\n",
    "    pipeline_log = Pipeline([\n",
    "            ('clf', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "    # tunning parameters\n",
    "    params_log = {\n",
    "        'clf__C': (1, 10, 100),\n",
    "#         'clf__multi_class': ('ovr', 'multinomial'),\n",
    "#         'clf__penalty': ('l1', 'l2')\n",
    "    }\n",
    "\n",
    "    # grid search\n",
    "    grid_log = GridSearchCV(\n",
    "        pipeline_log,        # pipeline from above\n",
    "        params_log,          # parameters to tune via cross validation\n",
    "        refit=True,          # fit using all available data at the end, on the best found param combination\n",
    "        n_jobs=-1,           # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "        scoring='accuracy',  # what score are we optimizing?\n",
    "        cv=StratifiedKFold(y_train, n_folds=n_folds),  # what type of cross validation to use\n",
    "#         cv=n_folds\n",
    "    )\n",
    "\n",
    "    return grid_log\n",
    "\n",
    "\n",
    "# ****************************************************************************\n",
    "# ****************************************************************************\n",
    "\n",
    "# Define Support Vector Machine\n",
    "# return: gridsearch SVM\n",
    "def SVM(y_train, n_folds=10):\n",
    "    # putting the steps explicitly into Pipeline\n",
    "    pipeline_svm = Pipeline([\n",
    "            # train on vectors with classifier\n",
    "            ('clf', SVC())\n",
    "        ])\n",
    "\n",
    "    # tunning parameters\n",
    "    params_svm = {\n",
    "        'clf__kernel': ('linear', 'poly', 'rbf'),\n",
    "        'clf__gamma': (0.00001, 0.0001, 00.1),\n",
    "        'clf__C': (1, 10, 100),\n",
    "    }\n",
    "\n",
    "    # grid search\n",
    "    grid_svm = GridSearchCV(\n",
    "        pipeline_svm,        # pipeline from above\n",
    "        params_svm,          # parameters to tune via cross validation\n",
    "        refit=True,          # fit using all available data at the end, on the best found param combination\n",
    "        n_jobs=-1,           # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "        scoring='accuracy',  # what score are we optimizing?\n",
    "        cv=StratifiedKFold(y_train, n_folds=n_folds),  # what type of cross validation to use\n",
    "    )\n",
    "\n",
    "    return grid_svm\n",
    "\n",
    "\n",
    "\n",
    "# ****************************************************************************\n",
    "# ****************************************************************************\n",
    "\n",
    "# Define Decision Trees\n",
    "def DT(y_train, n_folds=10):\n",
    "    # putting the steps explicitly into Pipeline\n",
    "    pipeline_dt = Pipeline([\n",
    "            # train on vectors with classifier\n",
    "            ('clf', RandomForestClassifier(criterion='entropy'))\n",
    "        ])\n",
    "\n",
    "    # tunning parameters\n",
    "    params_dt = {\n",
    "        'clf__n_estimators': (5, 10, 20, 50),\n",
    "        'clf__max_depth': (50, 150, 250),\n",
    "        'clf__min_samples_split': (1, 2, 3),\n",
    "        'clf__min_samples_leaf': (1, 2, 3)\n",
    "    }\n",
    "\n",
    "    # grid search\n",
    "    grid_dt = GridSearchCV(\n",
    "        pipeline_dt,        # pipeline from above\n",
    "        params_dt,          # parameters to tune via cross validation\n",
    "        refit=True,          # fit using all available data at the end, on the best found param combination\n",
    "        n_jobs=-1,           # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "        scoring='accuracy',  # what score are we optimizing?\n",
    "        cv=StratifiedKFold(y_train, n_folds=n_folds),  # what type of cross validation to use\n",
    "    )\n",
    "\n",
    "    return grid_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelTrain(X, y, model, n_folds=10, test_size=0.2):\n",
    "    # ***************************************************\n",
    "    # [Step 2]: Data Split(train=0.8, test=0.2)\n",
    "    # ***************************************************\n",
    "\n",
    "    date_now = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()).replace(\" \", \"_\")\n",
    "\n",
    "    # Split data Train and Test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=test_size)\n",
    "\n",
    "    print(\n",
    "        \"\\n\\n### DATA ##################################\\n\",\n",
    "        \"\\n\\tTrain data: \\t\", len(X_train),\n",
    "        \"\\n\\tTest data: \\t\", len(X_test),\n",
    "        \"\\n\\tAll data: \\t\", len(y_train) + len(y_test)\n",
    "    )\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 3]: Define Classifier\n",
    "    # ***************************************************\n",
    "    \n",
    "    if model == 'LOG':\n",
    "        grid_search = Log(y_train, n_folds)\n",
    "    elif model == 'SVM':\n",
    "        grid_search = SVM(y_train, n_folds)\n",
    "    elif model == 'DT':\n",
    "        grid_search = DT(y_train, n_folds)\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 4]: Compute Classifier\n",
    "    # ***************************************************\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    # fitting training sets to classifier\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # ***************************************************\n",
    "    # [Step 4]: Print Classifier Details\n",
    "    # ***************************************************\n",
    "\n",
    "    # print trained parameters\n",
    "    DetecterParams(grid_search, title=model, all_tunes=False)\n",
    "\n",
    "    # print computed time\n",
    "#     print(\"\\n\\n### COMPUTED TIME #########################\\n\")\n",
    "#     taken_time = time() - start_time\n",
    "#     print(\"[Started Time]: \", date_now)\n",
    "#     print(\"\\n[Taken Time]: \", str(datetime.timedelta(seconds=taken_time)))\n",
    "\n",
    "    # print classifier test results\n",
    "#     DetecterMetrics(X_train, y_train, grid_search, title=model + \": Train\")\n",
    "#     DetecterMetrics(X_test, y_test, grid_search, title=model + \": Test\")\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### DATA ##################################\n",
      " \n",
      "\tTrain data: \t 128 \n",
      "\tTest data: \t 0 \n",
      "\tAll data: \t 128\n",
      "\n",
      "\n",
      "### PARAMS ################################\n",
      "\n",
      "[LOG Detecter Params]: \n",
      "\n",
      "Best Score:  0.7421875\n",
      "Best Params:  {'clf__C': 1}\n",
      "\n",
      "\n",
      "### DATA ##################################\n",
      " \n",
      "\tTrain data: \t 128 \n",
      "\tTest data: \t 0 \n",
      "\tAll data: \t 128\n",
      "\n",
      "\n",
      "### PARAMS ################################\n",
      "\n",
      "[SVM Detecter Params]: \n",
      "\n",
      "Best Score:  0.7578125\n",
      "Best Params:  {'clf__kernel': 'rbf', 'clf__gamma': 0.1, 'clf__C': 1}\n",
      "\n",
      "\n",
      "### DATA ##################################\n",
      " \n",
      "\tTrain data: \t 177 \n",
      "\tTest data: \t 0 \n",
      "\tAll data: \t 177\n",
      "\n",
      "\n",
      "### PARAMS ################################\n",
      "\n",
      "[LOG Detecter Params]: \n",
      "\n",
      "Best Score:  0.553672316384\n",
      "Best Params:  {'clf__C': 10}\n",
      "\n",
      "\n",
      "### DATA ##################################\n",
      " \n",
      "\tTrain data: \t 177 \n",
      "\tTest data: \t 0 \n",
      "\tAll data: \t 177\n",
      "\n",
      "\n",
      "### PARAMS ################################\n",
      "\n",
      "[SVM Detecter Params]: \n",
      "\n",
      "Best Score:  0.54802259887\n",
      "Best Params:  {'clf__kernel': 'poly', 'clf__gamma': 0.1, 'clf__C': 100}\n"
     ]
    }
   ],
   "source": [
    "# ***************************************************\n",
    "# [Step 2]: Data Prepare\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "# Create DFs for models\n",
    "# WL: win, lose; WLD: win, lose, draw\n",
    "dfWL = CreateDfModel(draw=False, ht_draw=False)\n",
    "dfWLD = CreateDfModel(draw=True, ht_draw=False)\n",
    "\n",
    "# 'home_win': 1, 'away_win': 0, 'draw': 2\n",
    "dfWLD.groupby('result').mean()\n",
    "\n",
    "# Prepare Data for Logistic Regression\n",
    "X, y = CreateXy(dfWL, team_name=False, hash_emolex=True, score_ht=True, be_4game_sum=True)\n",
    "\n",
    "# Train Model\n",
    "log = ModelTrain(X, y, model='LOG', n_folds=10, test_size=0.0)\n",
    "svm = ModelTrain(X, y, model='SVM', n_folds=10, test_size=0.0)\n",
    "# ModelTrain(X, y, model='DT', n_folds=30, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Data for Logistic Regression\n",
    "X, y = CreateXy(dfWLD, team_name=False, hash_emolex=True, score_ht=True, be_4game_sum=True)\n",
    "\n",
    "# Train Model\n",
    "log = ModelTrain(X, y, model='LOG', n_folds=10, test_size=0.0)\n",
    "svm = ModelTrain(X, y, model='SVM', n_folds=10, test_size=0.0)\n",
    "# ModelTrain(X, y, model='DT', n_folds=30, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.pipeline.Pipeline.score>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
