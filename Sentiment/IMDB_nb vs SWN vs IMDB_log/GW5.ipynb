{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.chdir('/Users/Bya/git/predictEPL/MyFunctions/')\n",
    "import tokenizers\n",
    "import senti_word_net\n",
    "import featx\n",
    "import replacers\n",
    "import dataIO\n",
    "import converter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "os.chdir('/Users/Bya/Dropbox/Research/datas/movieclassifier/')\n",
    "from vectorizer import vect\n",
    "\n",
    "\n",
    "# load Long IMDB model\n",
    "clf = pickle.load(open(\n",
    "        os.path.join('pkl_objects',\n",
    "                     'classifier.pkl'), 'rb'))\n",
    "\n",
    "# load Short IMDB model\n",
    "with open(\"/Users/Bya/git/predictEPL/NLTK/NLTK_TUTORIAL/pickled_algos/naiveBayes_for_short_reviews_NEW.pickle\", \"rb\") as saved_classifier_f:\n",
    "    nb_classifier = pickle.load(saved_classifier_f)\n",
    "\n",
    "def cleanHash(word):\n",
    "    if word[0] == '#':\n",
    "        return '#'\n",
    "    elif word[0] == '@':\n",
    "        return '@'\n",
    "    elif word[0:4] == 'http':\n",
    "        return 'http'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def sentiment_short_long_IMDB(tweet):\n",
    "    \n",
    "    # can't -> cannot, bya's -> bya is\n",
    "    replacer = replacers.RegexpReplacer()\n",
    "    tweet = replacer.replace(tweet)\n",
    "    \n",
    "    # Tweet tokenizer\n",
    "    words = TweetTokenizer().tokenize(tweet)\n",
    "    \n",
    "    # defining stopwords\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    english_stops_added = english_stops | {'.', ',', ':', ';', '#', '?', 'RT', '-', '@', 'rt'}\n",
    "    words = [word.lower() for word in words if word.lower() not in english_stops_added]\n",
    "    \n",
    "    # Stemmer\n",
    "    porter = PorterStemmer()\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    \n",
    "    # words = map(lambda word: cleanHash(word), words)\n",
    "    words = [cleanHash(word) for word in words]\n",
    "    \n",
    "    # defining stopwords\n",
    "    english_stops_added =  {'.', ',', '!', ':', ';', '#', '?', 'RT', '-', '@', 'rt', 'http'}\n",
    "    words = [word.lower() for word in words if word.lower() not in english_stops_added]\n",
    " \n",
    "\n",
    "    # Naive Bayes: IMDB Short Reviews   \n",
    "    probs = nb_classifier.prob_classify(featx.bag_of_words(words))\n",
    "    short_sent = probs.max()\n",
    "    short_pos = probs.prob('pos')\n",
    "    short_neg = probs.prob('neg')\n",
    "    \n",
    "    \n",
    "    # Naive Bayes: IMDB Long Reviews\n",
    "    label = {0: 'neg', 1:'pos'}\n",
    "    sentence = [' '.join(words)]\n",
    "    X = vect.transform(sentence)\n",
    "    long_sent = label[clf.predict(X)[0]]\n",
    "    long_pos = clf.predict_proba(X)[0][1]\n",
    "    long_neg = clf.predict_proba(X)[0][0]\n",
    "    \n",
    "    \n",
    "    return (short_sent, short_pos, short_neg,\n",
    "            long_sent, long_pos, long_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GW10/SingleGames/Arsenal_vs_Everton.csv',\n",
       " 'GW10/SingleGames/Bournemouth_vs_Tottenham.csv',\n",
       " 'GW10/SingleGames/Leicester_vs_Crystal.csv',\n",
       " 'GW10/SingleGames/Liverpool_vs_Southampton.csv',\n",
       " 'GW10/SingleGames/Norwich_vs_WestBromwich.csv',\n",
       " 'GW10/SingleGames/Stoke_vs_Watford.csv',\n",
       " 'GW10/SingleGames/Sunderland_vs_Newcastle.csv',\n",
       " 'GW10/SingleGames/United_vs_City.csv',\n",
       " 'GW10/SingleGames/Villa_vs_Swansea.csv',\n",
       " 'GW10/SingleGames/WestHam_vs_Chelsea.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change home directory\n",
    "pathData = \"/Users/Bya/Dropbox/Research/datas/\"\n",
    "os.chdir(pathData)\n",
    "\n",
    "filenames = dataIO.GetFilenames('GW' + str(10) + '/SingleGames')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GW10 Arsenal Everton\n",
      "20.10829520225525\n",
      "GW10 Bournemouth Tottenham\n",
      "11.920794010162354\n",
      "GW10 Leicester Crystal\n",
      "4.213562965393066\n",
      "GW10 Liverpool Southampton\n",
      "29.66409993171692\n",
      "GW10 Norwich WestBromwich\n",
      "3.836544990539551\n",
      "GW10 Stoke Watford\n",
      "4.5339131355285645\n",
      "GW10 Sunderland Newcastle\n",
      "16.681102991104126\n",
      "GW10 United City\n",
      "76.45412611961365\n",
      "GW10 Villa Swansea\n",
      "8.870604038238525\n",
      "GW10 WestHam Chelsea\n",
      "35.87384510040283\n"
     ]
    }
   ],
   "source": [
    "for filePath in filenames:\n",
    "    start_time = time.time()\n",
    "\n",
    "    GW = filePath.split('/')[0]\n",
    "    home_team = filePath[:-4].split('/')[2].split('_')[0]\n",
    "    away_team = filePath[:-4].split('/')[2].split('_')[2]\n",
    "    print(GW, home_team, away_team)\n",
    "\n",
    "    # Change home directory\n",
    "    pathData = \"/Users/Bya/Dropbox/Research/datas/\"\n",
    "    os.chdir(pathData)    \n",
    "\n",
    "    # read csv as df\n",
    "    df = dataIO.csv_dic_df(filePath)\n",
    "\n",
    "    # add 'time' column\n",
    "    df['time'] = df['date']\n",
    "    df.time = [converter.toSeconds(date) for date in df.time]\n",
    "\n",
    "    # set time\n",
    "    start = converter.toSeconds(df.date[0]) - int(df['date'][0][-13:-11])\n",
    "\n",
    "    # add 'ith_minute'\n",
    "    df['ith_minute'] = (df['time'] - start) / 60\n",
    "\n",
    "    short_sent = []\n",
    "    short_pos = []\n",
    "    short_neg = []\n",
    "    long_sent = []\n",
    "    long_pos = []\n",
    "    long_neg = []\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        tweet = row[1]['text']\n",
    "        x,y,z,a,b,c = sentiment_short_long_IMDB(tweet)\n",
    "        short_sent.append(x)\n",
    "        short_pos.append(y)\n",
    "        short_neg.append(z)\n",
    "        long_sent.append(a)\n",
    "        long_pos.append(b)\n",
    "        long_neg.append(c)\n",
    "\n",
    "    df['short_sent'] = short_sent\n",
    "    df['short_pos'] = short_pos\n",
    "    df['short_neg'] = short_neg\n",
    "    df['long_sent'] = long_sent\n",
    "    df['long_pos'] = long_pos\n",
    "    df['long_neg'] = long_neg  \n",
    "\n",
    "\n",
    "    savePath = '/Users/Bya/Dropbox/Research/datas/Results/NB_Short_Long/' + GW\n",
    "    dataIO.DFtoCSV(df, savePath, home_team + \"_vs_\" + away_team, False)\n",
    "\n",
    "    # print computing time\n",
    "    print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
