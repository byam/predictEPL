{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/Bya/git/predictEPL/MyFunctions/')\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenize: \n",
      "\n",
      " I love you, Munjuu. I want you \n",
      "\n",
      "After Tokenizer: \n",
      "\n",
      " ['I', 'love', 'you', 'Munjuu', 'I', 'want', 'you'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleText = \"I love you, Munjuu. I want you\"\n",
    "tokenizedText = tokenizers.bya_token(sampleText)\n",
    "\n",
    "print(\"Before Tokenize: \\n\\n %s \\n\" % sampleText)\n",
    "print(\"After Tokenizer: \\n\\n %s \\n\" % tokenizedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the tagger\n",
    "import pickle\n",
    "\n",
    "with open('/Users/Bya/nltk_data/taggers/treebank_aubt.pickle', 'rb') as f:\n",
    "    tagger_aubt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUBT tagger: \n",
      "\n",
      " [('I', 'PRP'), ('love', 'VB'), ('you', 'PRP'), ('Munjuu', 'NN'), ('I', 'PRP'), ('want', 'VBP'), ('you', 'PRP')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "taggedAUBTwords = tagger_aubt.tag(tokenizedText)\n",
    "\n",
    "print(\"AUBT tagger: \\n\\n %s \\n\" % taggedAUBTwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With WordNet tags: \n",
      " \n",
      " [('i', None), ('love', 'v'), ('you', None), ('munjuu', 'n'), ('i', None), ('want', 'v'), ('you', None)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def wordnet_sanitize(word):\n",
    "    \"\"\"\n",
    "    Ensure that word is a (string, pos) pair that WordNet can understand.\n",
    " \n",
    "    Argument: word (str, str) -- a (string, pos) pair\n",
    " \n",
    "    Value: a possibly modified (string, pos) pair, where pos=None if\n",
    "    the input pos is outside of WordNet.\n",
    "    \"\"\"\n",
    "    string, tag = word\n",
    "    string = string.lower()\n",
    "    tag = tag.lower()\n",
    "    if tag.startswith('v'):    tag = 'v'\n",
    "    elif tag.startswith('n'):  tag = 'n'\n",
    "    elif tag.startswith('j'):  tag = 'a'\n",
    "    elif tag.startswith('rb'): tag = 'r'\n",
    "    if tag in ('a', 'n', 'r', 'v'):\n",
    "        return (string, tag)\n",
    "    else:\n",
    "        return (string, None)\n",
    "\n",
    "wordsWNtag = list(map(lambda word: wordnet_sanitize(word), taggedAUBTwords))\n",
    "\n",
    "print(\"With WordNet tags: \\n \\n %s \\n\" % wordsWNtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35, 0.090625, 0.559375)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def senti_word_net(word):\n",
    "    pos_score = 0.0\n",
    "    neg_score = 0.0\n",
    "    obj_score = 0.0\n",
    "    \n",
    "    string, tag = word\n",
    "    \n",
    "    if tag is None:\n",
    "        return pos_score, neg_score, obj_score\n",
    "    \n",
    "    wordList = list(swn.senti_synsets(string, tag))\n",
    "\n",
    "    word_num = len(wordList)\n",
    "\n",
    "    if word_num:\n",
    "        for word in wordList:\n",
    "            pos_score += word.pos_score()\n",
    "            neg_score += word.neg_score()\n",
    "            obj_score += word.obj_score()\n",
    "        \n",
    "        return pos_score/word_num, neg_score/word_num, obj_score/word_num\n",
    "\n",
    "    return pos_score, neg_score, obj_score\n",
    "\n",
    "def pos_neg_score(words):\n",
    "    pos_score = 0.0\n",
    "    neg_score = 0.0\n",
    "    obj_score = 0.0\n",
    "    \n",
    "    senti_words = 0\n",
    "    \n",
    "    for word in words:\n",
    "        p, n, o = senti_word_net(word)\n",
    "        \n",
    "        if p:\n",
    "            pos_score += p\n",
    "            neg_score += n\n",
    "            obj_score += o\n",
    "            \n",
    "            senti_words += 1\n",
    "    \n",
    "    return pos_score/senti_words, neg_score/senti_words, obj_score/senti_words\n",
    "\n",
    "pos_neg_score(wordsWNtag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
