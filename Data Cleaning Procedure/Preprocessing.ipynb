{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/Bya/git/predictEPL/MyFunctions/\")\n",
    "from dataIO import GetFilenames, csv_dic_df\n",
    "from converter import toSeconds\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import replacers\n",
    "\n",
    "def cleanHash(word):\n",
    "    if word[0] == '#':\n",
    "        return word[1::]\n",
    "    elif word[0] == '@':\n",
    "        return '@'\n",
    "    elif word[0:4] == 'http':\n",
    "        return 'http'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def preprocessing_tweet(tweet, debug = False):\n",
    "    if debug:\n",
    "        print(\"====================================\")\n",
    "        print(\"[Original Tweet]: \\n\\n %s \\n\\n\" % tweet)\n",
    "\n",
    "\n",
    "    # can't -> cannot, bya's -> bya is\n",
    "    replacer = replacers.RegexpReplacer()\n",
    "    tweet = replacer.replace(tweet)\n",
    "    if debug:\n",
    "        print(\"====================================\")\n",
    "        print(\"[Replaced Tweet]: \\n\\n %s \\n\\n\" % tweet)\n",
    "\n",
    "\n",
    "    # Tweet tokenizer and lower case\n",
    "    words = TweetTokenizer().tokenize(tweet)\n",
    "    words = [word.lower() for word in words]\n",
    "    if debug:\n",
    "        print(\"====================================\")\n",
    "        print(\"[Tokenized Tweet]: \\n\\n %s \\n\\n\" % words)\n",
    "\n",
    "\n",
    "    # defining stopwords\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    english_stops_added = english_stops | {'!', '.', ',', ':', ';', '#', '?', 'RT', '-', '@', 'rt'}\n",
    "    words = [word for word in words if word not in english_stops_added]\n",
    "    if debug:\n",
    "        print(\"====================================\")\n",
    "        print(\"[Cleaned Stopwords Tweet]: \\n\\n %s \\n\\n\" % words)\n",
    "\n",
    "\n",
    "\n",
    "    # words = map(lambda word: cleanHash(word), words)\n",
    "    words = [cleanHash(word) for word in words]\n",
    "    if debug:\n",
    "        print(\"====================================\")\n",
    "        print(\"[Clean hash Tweet]: \\n\\n %s \\n\\n\" % words)\n",
    "\n",
    "\n",
    "\n",
    "    # Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words_stemmed = list(map(lambda word: stemmer.stem(word), words))\n",
    "    if debug:\n",
    "        print(\"====================================\")\n",
    "        print(\"[Stemmed hash Tweet]: \\n\\n %s \\n\\n\" % words_stemmed)\n",
    "\n",
    "\n",
    "\n",
    "    # Cleaning not useful Tweets\n",
    "    # ex: Watch Live Stream\n",
    "    pattern1 = [\"watch\", \"live\", \"stream\"]\n",
    "    good_tweet = True\n",
    "    if pattern1[0] in words and pattern1[1] in words and pattern1[2] in words:\n",
    "        good_tweet = False\n",
    "        if debug:\n",
    "            print(\"====================================\")\n",
    "            print(\"[Not good Tweet!]: \\n\\n detected: \\n\\n %s\" % pattern1)\n",
    "    \n",
    "    return words, words_stemmed, good_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "[Original Tweet]: \n",
      "\n",
      " Every time there's a corner against us recently it feels like we're going to concede #efc \n",
      "\n",
      "\n",
      "====================================\n",
      "[Replaced Tweet]: \n",
      "\n",
      " Every time there is a corner against us recently it feels like we are going to concede #efc \n",
      "\n",
      "\n",
      "====================================\n",
      "[Tokenized Tweet]: \n",
      "\n",
      " ['every', 'time', 'there', 'is', 'a', 'corner', 'against', 'us', 'recently', 'it', 'feels', 'like', 'we', 'are', 'going', 'to', 'concede', '#efc'] \n",
      "\n",
      "\n",
      "====================================\n",
      "[Cleaned Stopwords Tweet]: \n",
      "\n",
      " ['every', 'time', 'corner', 'us', 'recently', 'feels', 'like', 'going', 'concede', '#efc'] \n",
      "\n",
      "\n",
      "====================================\n",
      "[Clean hash Tweet]: \n",
      "\n",
      " ['every', 'time', 'corner', 'us', 'recently', 'feels', 'like', 'going', 'concede', 'efc'] \n",
      "\n",
      "\n",
      "====================================\n",
      "[Stemmed hash Tweet]: \n",
      "\n",
      " ['everi', 'time', 'corner', 'us', 'recent', 'feel', 'like', 'go', 'conced', 'efc'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet = \"Every time there's a corner against us recently it feels like we're going to concede #efc\"\n",
    "\n",
    "words, words_stemmed, good_tweet = preprocessing_tweet(tweet, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
