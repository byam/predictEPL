{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/utils/\")\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/config/\")\n",
    "\n",
    "import emolex\n",
    "import paths\n",
    "import tokenizer\n",
    "import useful_methods as my_methods\n",
    "import train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose : DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Words: 14136\n",
      "POS:  3207\n",
      "NEG:  4008\n",
      "\n",
      "All:  7215\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => Soccer Hash Emolex\n",
    "dic_emolex_soccer, y = emolex.EmolexSoccerDic()\n",
    "\n",
    "# Read Hash Emolex CSV\n",
    "dfHashEmolex = train_datas.HashEmolexAllRead()\n",
    "\n",
    "# Labeling Emolex 8 cat => POS, NEG\n",
    "texts = []\n",
    "sentiments = []\n",
    "\n",
    "for i in range(len(dfHashEmolex)):\n",
    "    text = dfHashEmolex.iloc[i]['text']\n",
    "    sentiment = dfHashEmolex.iloc[i]['sentiments'].split(',')\n",
    "    \n",
    "    for sent in sentiment:\n",
    "        if sent in ['anger', 'disgust', 'fear', 'sadness', 'negative']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(0) # 0 is negative\n",
    "            break\n",
    "\n",
    "        elif sent in ['anticipation', 'joy', 'positive', 'trust']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(1) # 1 is negative\n",
    "            break\n",
    "\n",
    "# Create New POS, NEG dataframe\n",
    "dfTwitter = pd.DataFrame(columns=['tweet', 'sentiment'])\n",
    "dfTwitter['tweet'] = texts\n",
    "dfTwitter['sentiment'] = sentiments\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]\n",
    "\n",
    "print(\"POS: \", len(dfTwitter[dfTwitter.sentiment == 1]))\n",
    "print(\"NEG: \", len(dfTwitter[dfTwitter.sentiment == 0]))\n",
    "print(\"\\nAll: \", len(dfTwitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  5772 \n",
      "Test data:  1443 \n",
      "All data:  7215\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 2: Split Datas\n",
    "\n",
    "\n",
    "# Split data Train and Test data\n",
    "tweets_train, tweets_test, sentiment_train, sentiment_test = \\\n",
    "    train_test_split(dfTwitter['tweet'], dfTwitter['sentiment'], test_size=0.2)\n",
    "\n",
    "print(\"Train data: \", len(tweets_train), \"\\nTest data: \", len(tweets_test),\n",
    "      \"\\nAll data: \", len(sentiment_train) + len(sentiment_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# Step 3: Set Parameters for Classifier\n",
    "\n",
    "# Classifier Pipeline\n",
    "pipeline = Pipeline([\n",
    "   ('vect', TfidfVectorizer()),\n",
    "   ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Hyper Parameters\n",
    "parameters = {\n",
    "    'vect__analyzer': (\n",
    "#         tokenizer.StemNoStops,\n",
    "#                        tokenizer.StemNoEnglishStops,\n",
    "#                        tokenizer.StemNoSoccerStops,\n",
    "#                        tokenizer.StemNoNegation,\n",
    "                       tokenizer.Stem,\n",
    "#                        tokenizer.LemmaNoStops,\n",
    "#                        tokenizer.LemmaNoEnglishStops,\n",
    "#                        tokenizer.LemmaNoSoccerStops,\n",
    "#                        tokenizer.LemmaNoNegation,\n",
    "                       tokenizer.Lemma),\n",
    "    'vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'vect__max_features': (2500, 5000, 10000, None),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'vect__use_idf': (True, False),\n",
    "    'vect__norm': ('l1', 'l2'),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__C': (0.01, 0.1, 1, 10),\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,  # pipeline from above\n",
    "    parameters,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(sentiment_train, n_folds=5),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.1 s, sys: 6.48 s, total: 52.6 s\n",
      "Wall time: 3h 18min 59s\n",
      "Best score: 0.886\n",
      "Best parameters set:\n",
      "\tclf__C: 10\n",
      "\tclf__penalty: 'l1'\n",
      "\tvect__analyzer: <function Lemma at 0x10736b378>\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__max_features: 5000\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__norm: 'l2'\n",
      "\n",
      "\n",
      "\n",
      "[Test Results]:\n",
      "\n",
      "Accuracy: 0.878724878725\n",
      "Precision: 0.873040752351\n",
      "Recall: 0.855606758833\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "# Step 4: Compute Classifier\n",
    "\n",
    "%time grid_search.fit(tweets_train, sentiment_train)\n",
    "\n",
    "print('Best score: %0.3f' % grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "   print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\n",
    "predictions = grid_search.predict(tweets_test)\n",
    "print(\"\\n\\n\\n[Test Results]:\\n\")\n",
    "print('Accuracy:', accuracy_score(sentiment_test, predictions))\n",
    "print('Precision:', precision_score(sentiment_test, predictions))\n",
    "print('Recall:', recall_score(sentiment_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10,\n",
       " 'clf__penalty': 'l1',\n",
       " 'vect__analyzer': <function tokenizer.Lemma>,\n",
       " 'vect__max_df': 0.25,\n",
       " 'vect__max_features': 5000,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': 'l2'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Best parameters set:')\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.83056, std: 0.01476, params: {'vect__ngram_range': (1, 1), 'vect__analyzer': <function StemNoStops at 0x1072c8e18>},\n",
      " mean: 0.83056, std: 0.01476, params: {'vect__ngram_range': (1, 2), 'vect__analyzer': <function StemNoStops at 0x1072c8e18>},\n",
      " mean: 0.85031, std: 0.01524, params: {'vect__ngram_range': (1, 1), 'vect__analyzer': <function StemNoNegation at 0x10736b268>},\n",
      " mean: 0.85031, std: 0.01524, params: {'vect__ngram_range': (1, 2), 'vect__analyzer': <function StemNoNegation at 0x10736b268>},\n",
      " mean: 0.85100, std: 0.01335, params: {'vect__ngram_range': (1, 1), 'vect__analyzer': <function Stem at 0x10736b2f0>},\n",
      " mean: 0.85100, std: 0.01335, params: {'vect__ngram_range': (1, 2), 'vect__analyzer': <function Stem at 0x10736b2f0>},\n",
      " mean: 0.83039, std: 0.01778, params: {'vect__ngram_range': (1, 1), 'vect__analyzer': <function LemmaNoStops at 0x10736b400>},\n",
      " mean: 0.83039, std: 0.01778, params: {'vect__ngram_range': (1, 2), 'vect__analyzer': <function LemmaNoStops at 0x10736b400>},\n",
      " mean: 0.85031, std: 0.01413, params: {'vect__ngram_range': (1, 1), 'vect__analyzer': <function LemmaNoNegation at 0x10736b598>},\n",
      " mean: 0.85031, std: 0.01413, params: {'vect__ngram_range': (1, 2), 'vect__analyzer': <function LemmaNoNegation at 0x10736b598>},\n",
      " mean: 0.84910, std: 0.01394, params: {'vect__ngram_range': (1, 1), 'vect__analyzer': <function Lemma at 0x10736b378>},\n",
      " mean: 0.84910, std: 0.01394, params: {'vect__ngram_range': (1, 2), 'vect__analyzer': <function Lemma at 0x10736b378>}]\n"
     ]
    }
   ],
   "source": [
    "pprint(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 3: Save Detecter\n",
    "\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('log_tweets_pn_detector-160114.pkl', 'wb') as fout:\n",
    "    pickle.dump(grid_search, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
