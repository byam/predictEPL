{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/utils/\")\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/config/\")\n",
    "\n",
    "import emolex\n",
    "import paths\n",
    "import tokenizer\n",
    "import useful_methods as my_methods\n",
    "import train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose : DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emolex Dic's All Words]: 14136\n",
      "POS:  3207\n",
      "NEG:  4008\n",
      "\n",
      "All:  7215\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => Soccer Hash Emolex\n",
    "dic_emolex_soccer, y = emolex.EmolexSoccerDic()\n",
    "\n",
    "# Read Hash Emolex CSV\n",
    "dfHashEmolex = train_datas.HashEmolexAllRead()\n",
    "\n",
    "# Labeling Emolex 8 cat => POS, NEG\n",
    "texts = []\n",
    "sentiments = []\n",
    "\n",
    "for i in range(len(dfHashEmolex)):\n",
    "    text = dfHashEmolex.iloc[i]['text']\n",
    "    sentiment = dfHashEmolex.iloc[i]['sentiments'].split(',')\n",
    "    \n",
    "    for sent in sentiment:\n",
    "        if sent in ['anger', 'disgust', 'fear', 'sadness', 'negative']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(0) # 0 is negative\n",
    "            break\n",
    "\n",
    "        elif sent in ['anticipation', 'joy', 'positive', 'trust']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(1) # 1 is negative\n",
    "            break\n",
    "\n",
    "# Create New POS, NEG dataframe\n",
    "dfTwitter = pd.DataFrame(columns=['tweet', 'sentiment'])\n",
    "dfTwitter['tweet'] = texts\n",
    "dfTwitter['sentiment'] = sentiments\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]\n",
    "\n",
    "print(\"POS: \", len(dfTwitter[dfTwitter.sentiment == 1]))\n",
    "print(\"NEG: \", len(dfTwitter[dfTwitter.sentiment == 0]))\n",
    "print(\"\\nAll: \", len(dfTwitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => SemEval Tweet PN\n",
    "\n",
    "# Read Data\n",
    "dfTwitter = train_datas.TweetPnEqualRead()\n",
    "\n",
    "# Change sentiment to number\n",
    "label_dic = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "dfTwitter.sentiment = [label_dic[sentiment] for sentiment in list(dfTwitter.sentiment)]\n",
    "\n",
    "# Adding 'length' column\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => Movie Short reviews\n",
    "\n",
    "os.chdir(paths.READ_PATH_REVIEW_SHORT)\n",
    "dfTwitter = my_methods.csv_dic_df(\"short_movie_reviews.csv\")\n",
    "\n",
    "# Adding 'length' column\n",
    "dfTwitter['tweet'] = dfTwitter['review']\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 2: Data to Vectors\n",
    "\n",
    "# Bag Of Word(bof), Porter Stemmer\n",
    "print(\"Bag Of Word(bof), Porter Stemmer:\")\n",
    "%time bow_transformer = CountVectorizer(analyzer=tokenizer.Stem).fit(dfTwitter['tweet'])\n",
    "\n",
    "\n",
    "\n",
    "# The bag-of-words counts for the entire Tweets corpus are a large,\n",
    "# sparse matrix:\n",
    "tweets_bow = bow_transformer.transform(dfTwitter['tweet'])\n",
    "print('\\nsparse matrix shape:', tweets_bow.shape)\n",
    "print('number of non-zeros:', tweets_bow.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * tweets_bow.nnz / (tweets_bow.shape[0] * tweets_bow.shape[1])))\n",
    "\n",
    "\n",
    "\n",
    "# And finally, after the counting,\n",
    "# the term weighting and normalization can be done with TF-IDF,\n",
    "# using scikit-learn's TfidfTransformer:\n",
    "tfidf_transformer = TfidfTransformer().fit(tweets_bow)\n",
    "\n",
    "\n",
    "# To transform the entire bag-of-words corpus into TF-IDF corpus at once:\n",
    "tweets_tfidf = tfidf_transformer.transform(tweets_bow)\n",
    "print(\"\\nTF-IDF(bow)\\n\", tweets_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  5772 \n",
      "Test data:  1443 \n",
      "All data:  7215\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 3: Run Expirements & Tune Params\n",
    "\n",
    "\n",
    "# Split data Train and Test data\n",
    "tweets_train, tweets_test, sentiment_train, sentiment_test = \\\n",
    "    train_test_split(dfTwitter['tweet'], dfTwitter['sentiment'], test_size=0.2)\n",
    "\n",
    "print(\"Train data: \", len(tweets_train), \"\\nTest data: \", len(tweets_test),\n",
    "      \"\\nAll data: \", len(sentiment_train) + len(sentiment_test))\n",
    "\n",
    "\n",
    "\n",
    "# Let's recap the entire pipeline up to this point,\n",
    "# putting the steps explicitly into scikit-learn's Pipeline:\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=tokenizer.Stem)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Tune Parameters\n",
    "params = {\n",
    "#     'tfidf__use_idf': (True),\n",
    "    'bow__analyzer': (tokenizer.StemNoStops, tokenizer.LemmaNoStops,\n",
    "                      tokenizer.StemNoEnglishStops, tokenizer.LemmaNoEnglishStops,\n",
    "                      tokenizer.StemNoSoccerStops, tokenizer.LemmaNoSoccerStops,\n",
    "                      tokenizer.StemNoNegation, tokenizer.LemmaNoNegation,\n",
    "                      tokenizer.Stem, tokenizer.Lemma),\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,  # pipeline from above\n",
    "    params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(sentiment_train, n_folds=10),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's recap the entire pipeline up to this point,\n",
    "# putting the steps explicitly into scikit-learn's Pipeline:\n",
    "pipelineLog = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=tokenizer.Stem)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Tune Parameters\n",
    "params = {\n",
    "#     'tfidf__use_idf': (True),\n",
    "    'bow__analyzer': (tokenizer.StemNoStops, tokenizer.LemmaNoStops,\n",
    "                      tokenizer.StemNoEnglishStops, tokenizer.LemmaNoEnglishStops,\n",
    "                      tokenizer.StemNoSoccerStops, tokenizer.LemmaNoSoccerStops,\n",
    "                      tokenizer.StemNoNegation, tokenizer.LemmaNoNegation,\n",
    "                      tokenizer.Stem, tokenizer.Lemma),\n",
    "}\n",
    "\n",
    "gridLog = GridSearchCV(\n",
    "    pipelineLog,  # pipeline from above\n",
    "    params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(sentiment_train, n_folds=10),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "%time nb_detector = grid.fit(tweets_train, sentiment_train)\n",
    "pprint(nb_detector.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_detector.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = nb_detector.predict(tweets_test)\n",
    "print(\"[Test Results]:\\n\")\n",
    "print('Accuracy:', accuracy_score(sentiment_test, predictions))\n",
    "print('Precision:', precision_score(sentiment_test, predictions))\n",
    "print('Recall:', recall_score(sentiment_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.89 s, sys: 449 ms, total: 8.34 s\n",
      "Wall time: 5min 3s\n",
      "[mean: 0.84459, std: 0.01701, params: {'bow__analyzer': <function StemNoStops at 0x110b03e18>},\n",
      " mean: 0.84200, std: 0.01326, params: {'bow__analyzer': <function LemmaNoStops at 0x110b9a620>},\n",
      " mean: 0.84737, std: 0.01336, params: {'bow__analyzer': <function StemNoEnglishStops at 0x110b030d0>},\n",
      " mean: 0.84806, std: 0.01510, params: {'bow__analyzer': <function LemmaNoEnglishStops at 0x110b9a6a8>},\n",
      " mean: 0.85707, std: 0.01520, params: {'bow__analyzer': <function StemNoSoccerStops at 0x110b9a488>},\n",
      " mean: 0.85915, std: 0.01036, params: {'bow__analyzer': <function LemmaNoSoccerStops at 0x110b9a730>},\n",
      " mean: 0.85932, std: 0.01515, params: {'bow__analyzer': <function StemNoNegation at 0x110b9a400>},\n",
      " mean: 0.85949, std: 0.01350, params: {'bow__analyzer': <function LemmaNoNegation at 0x110b9a7b8>},\n",
      " mean: 0.85811, std: 0.01566, params: {'bow__analyzer': <function Stem at 0x110b9a510>},\n",
      " mean: 0.85620, std: 0.01456, params: {'bow__analyzer': <function Lemma at 0x110b9a598>}]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "%time log_detector = gridLog.fit(tweets_train, sentiment_train)\n",
    "pprint(log_detector.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bow__analyzer': <function tokenizer.LemmaNoNegation>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_detector.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = log_detector.predict(tweets_test)\n",
    "print(\"[Test Results]:\\n\")\n",
    "print('Accuracy:', accuracy_score(sentiment_test, predictions))\n",
    "print('Precision:', precision_score(sentiment_test, predictions))\n",
    "print('Recall:', recall_score(sentiment_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 3: Save Detecter\n",
    "\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('log_tweets_pn_detector-20160114.pkl', 'wb') as fout:\n",
    "    pickle.dump(log_detector, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Detecter\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('log_tweets_pn_detector-20160114.pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'utf-8'\n",
    "    log_detector_reloaded = u.load()\n",
    "    classifier = log_detector_reloaded\n",
    "    print(log_detector_reloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in range(10):\n",
    "    tweet = dfTwitter.tweet[index]\n",
    "    label = dfTwitter.sentiment[index]\n",
    "    print(\"\\n\\n================================\")\n",
    "    print(\"[Tweet]:\\n\", tweet)\n",
    "    print(\"[Sentiment]: \", label)\n",
    "    print(\"\\n[Classifier]:\")\n",
    "    print (classifier.predict_proba(tweet)[0], classifier.predict(tweet)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Receiver Operating Characteristic = ROC curve\n",
    "# Visualizes a classifier's performance\n",
    "# for all values of the discrimination threshold. \n",
    "\n",
    "# fall out: F = FP / (TN + FP)\n",
    "\n",
    "# AUC (area under the curve)\n",
    "\n",
    "predictions_test = nb_detector.predict_proba(tweets_test)\n",
    "\n",
    "false_positive_rate, recall, thresholds = roc_curve(\n",
    "    sentiment_test, predictions_test[:, 1])\n",
    "\n",
    "roc_auc = auc(false_positive_rate, recall)\n",
    "\n",
    "# Plot\n",
    "plt.title('[NB]: Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Fall-out')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Receiver Operating Characteristic = ROC curve\n",
    "# Visualizes a classifier's performance\n",
    "# for all values of the discrimination threshold. \n",
    "\n",
    "# fall out: F = FP / (TN + FP)\n",
    "\n",
    "# AUC (area under the curve)\n",
    "\n",
    "predictions_test = log_detector.predict_proba(tweets_test)\n",
    "\n",
    "false_positive_rate, recall, thresholds = roc_curve(\n",
    "    sentiment_test, predictions_test[:, 1])\n",
    "\n",
    "roc_auc = auc(false_positive_rate, recall)\n",
    "\n",
    "# Plot\n",
    "plt.title('[Log]: Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Fall-out')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
