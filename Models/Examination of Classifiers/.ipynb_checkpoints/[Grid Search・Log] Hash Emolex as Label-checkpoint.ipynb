{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/utils/\")\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/config/\")\n",
    "\n",
    "import emolex\n",
    "import paths\n",
    "import tokenizer\n",
    "import useful_methods as my_methods\n",
    "import train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose : DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => Soccer Hash Emolex\n",
    "dic_emolex_soccer, y = emolex.EmolexSoccerDic()\n",
    "\n",
    "# Read Hash Emolex CSV\n",
    "dfHashEmolex = train_datas.HashEmolexAllRead()\n",
    "\n",
    "# Labeling Emolex 8 cat => POS, NEG\n",
    "texts = []\n",
    "sentiments = []\n",
    "\n",
    "for i in range(len(dfHashEmolex)):\n",
    "    text = dfHashEmolex.iloc[i]['text']\n",
    "    sentiment = dfHashEmolex.iloc[i]['sentiments'].split(',')\n",
    "    \n",
    "    for sent in sentiment:\n",
    "        if sent in ['anger', 'disgust', 'fear', 'sadness', 'negative']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(0) # 0 is negative\n",
    "            break\n",
    "\n",
    "        elif sent in ['anticipation', 'joy', 'positive', 'trust']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(1) # 1 is negative\n",
    "            break\n",
    "\n",
    "# Create New POS, NEG dataframe\n",
    "dfTwitter = pd.DataFrame(columns=['tweet', 'sentiment'])\n",
    "dfTwitter['tweet'] = texts\n",
    "dfTwitter['sentiment'] = sentiments\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]\n",
    "\n",
    "print(\"POS: \", len(dfTwitter[dfTwitter.sentiment == 1]))\n",
    "print(\"NEG: \", len(dfTwitter[dfTwitter.sentiment == 0]))\n",
    "print(\"\\nAll: \", len(dfTwitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b13ef7c2afee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split data Train and Test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtweets_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_test\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTwitter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfTwitter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m print(\"Train data: \", len(tweets_train), \"\\nTest data: \", len(tweets_test),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 2: Split Datas\n",
    "\n",
    "\n",
    "# Split data Train and Test data\n",
    "tweets_train, tweets_test, sentiment_train, sentiment_test = \\\n",
    "    train_test_split(dfTwitter['tweet'], dfTwitter['sentiment'], test_size=0.2)\n",
    "\n",
    "print(\"Train data: \", len(tweets_train), \"\\nTest data: \", len(tweets_test),\n",
    "      \"\\nAll data: \", len(sentiment_train) + len(sentiment_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# Step 3: Set Parameters for Classifier\n",
    "\n",
    "# Classifier Pipeline\n",
    "pipeline = Pipeline([\n",
    "   ('vect', TfidfVectorizer()),\n",
    "   ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Hyper Parameters\n",
    "parameters = {\n",
    "    'vect__analyzer': (\n",
    "#                     tokenizer.StemNoStops,\n",
    "#                        tokenizer.StemNoEnglishStops,\n",
    "#                        tokenizer.StemNoSoccerStops,\n",
    "                       tokenizer.StemNoNegation,\n",
    "                       tokenizer.Stem,\n",
    "#                        tokenizer.LemmaNoStops,\n",
    "#                        tokenizer.LemmaNoEnglishStops,\n",
    "#                        tokenizer.LemmaNoSoccerStops,\n",
    "                       tokenizer.LemmaNoNegation,\n",
    "                       tokenizer.Lemma),\n",
    "#     'vect__max_df': (0.25, 0.5, 0.75),\n",
    "#     'vect__max_features': (2500, 5000, 10000, None),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'vect__use_idf': (True, False),\n",
    "    'vect__norm': ('l1', 'l2'),\n",
    "#     'clf__penalty': ('l1', 'l2'),\n",
    "#     'clf__C': (0.01, 0.1, 1, 10),\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,  # pipeline from above\n",
    "    parameters,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(sentiment_train, n_folds=5),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Step 4: Compute Classifier\n",
    "\n",
    "grid_search.fit(tweets_train, sentiment_train)\n",
    "\n",
    "print('Best score: %0.3f' % grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "   print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\n",
    "predictions = grid_search.predict(tweets_test)\n",
    "print(\"\\n\\n\\n[Test Results]:\\n\")\n",
    "print('Accuracy:', accuracy_score(sentiment_test, predictions))\n",
    "print('Precision:', precision_score(sentiment_test, predictions))\n",
    "print('Recall:', recall_score(sentiment_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Best parameters set:')\n",
    "log_detector.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 3: Save Detecter\n",
    "\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('nb_tweets_pn_detector.pkl', 'wb') as fout:\n",
    "    pickle.dump(nb_detector, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
