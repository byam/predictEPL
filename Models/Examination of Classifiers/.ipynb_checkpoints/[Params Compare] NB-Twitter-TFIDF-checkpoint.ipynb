{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/utils/\")\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/config/\")\n",
    "\n",
    "import emolex\n",
    "import paths\n",
    "import tokenizer\n",
    "import useful_methods as my_methods\n",
    "import train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose : DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Words: 14136\n",
      "POS:  3207\n",
      "NEG:  4008\n",
      "\n",
      "All:  7215\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => Soccer Hash Emolex\n",
    "dic_emolex_soccer, y = emolex.EmolexSoccerDic()\n",
    "\n",
    "# Read Hash Emolex CSV\n",
    "dfHashEmolex = train_datas.HashEmolexAllRead()\n",
    "\n",
    "# Labeling Emolex 8 cat => POS, NEG\n",
    "texts = []\n",
    "sentiments = []\n",
    "\n",
    "for i in range(len(dfHashEmolex)):\n",
    "    text = dfHashEmolex.iloc[i]['text']\n",
    "    sentiment = dfHashEmolex.iloc[i]['sentiments'].split(',')\n",
    "    \n",
    "    for sent in sentiment:\n",
    "        if sent in ['anger', 'disgust', 'fear', 'sadness', 'negative']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(0) # 0 is negative\n",
    "            break\n",
    "\n",
    "        elif sent in ['anticipation', 'joy', 'positive', 'trust']:\n",
    "            texts.append(text)\n",
    "            sentiments.append(1) # 1 is negative\n",
    "            break\n",
    "\n",
    "# Create New POS, NEG dataframe\n",
    "dfTwitter = pd.DataFrame(columns=['tweet', 'sentiment'])\n",
    "dfTwitter['tweet'] = texts\n",
    "dfTwitter['sentiment'] = sentiments\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]\n",
    "\n",
    "print(\"POS: \", len(dfTwitter[dfTwitter.sentiment == 1]))\n",
    "print(\"NEG: \", len(dfTwitter[dfTwitter.sentiment == 0]))\n",
    "print(\"\\nAll: \", len(dfTwitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => SemEval Tweet PN\n",
    "\n",
    "# Read Data\n",
    "dfTwitter = train_datas.TweetPnEqualRead()\n",
    "\n",
    "# Change sentiment to number\n",
    "label_dic = {\n",
    "    'positive': 1,\n",
    "    'negative': 0,\n",
    "}\n",
    "\n",
    "dfTwitter.sentiment = [label_dic[sentiment] for sentiment in list(dfTwitter.sentiment)]\n",
    "\n",
    "# Adding 'length' column\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 1. Prepare Data => Movie Short reviews\n",
    "\n",
    "os.chdir(paths.READ_PATH_REVIEW_SHORT)\n",
    "dfTwitter = my_methods.csv_dic_df(\"short_movie_reviews.csv\")\n",
    "\n",
    "# Adding 'length' column\n",
    "dfTwitter['tweet'] = dfTwitter['review']\n",
    "dfTwitter['length'] = [len(text) for text in dfTwitter['tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag Of Word(bof), Porter Stemmer:\n",
      "CPU times: user 7.09 s, sys: 66.1 ms, total: 7.15 s\n",
      "Wall time: 7.26 s\n",
      "\n",
      "sparse matrix shape: (7215, 11955)\n",
      "number of non-zeros: 71839\n",
      "sparsity: 0.08%\n",
      "\n",
      "TF-IDF(bow)\n",
      " (7215, 11955)\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 2: Data to Vectors\n",
    "\n",
    "# Bag Of Word(bof), Porter Stemmer\n",
    "print(\"Bag Of Word(bof), Porter Stemmer:\")\n",
    "%time bow_transformer = CountVectorizer(analyzer=tokenizer.Stem).fit(dfTwitter['tweet'])\n",
    "\n",
    "\n",
    "\n",
    "# The bag-of-words counts for the entire Tweets corpus are a large,\n",
    "# sparse matrix:\n",
    "tweets_bow = bow_transformer.transform(dfTwitter['tweet'])\n",
    "print('\\nsparse matrix shape:', tweets_bow.shape)\n",
    "print('number of non-zeros:', tweets_bow.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * tweets_bow.nnz / (tweets_bow.shape[0] * tweets_bow.shape[1])))\n",
    "\n",
    "\n",
    "\n",
    "# And finally, after the counting,\n",
    "# the term weighting and normalization can be done with TF-IDF,\n",
    "# using scikit-learn's TfidfTransformer:\n",
    "tfidf_transformer = TfidfTransformer().fit(tweets_bow)\n",
    "\n",
    "\n",
    "# To transform the entire bag-of-words corpus into TF-IDF corpus at once:\n",
    "tweets_tfidf = tfidf_transformer.transform(tweets_bow)\n",
    "print(\"\\nTF-IDF(bow)\\n\", tweets_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  5772 \n",
      "Test data:  1443 \n",
      "All data:  7215\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Step 3: Run Expirements & Tune Params\n",
    "\n",
    "\n",
    "# Split data Train and Test data\n",
    "tweets_train, tweets_test, sentiment_train, sentiment_test = \\\n",
    "    train_test_split(dfTwitter['tweet'], dfTwitter['sentiment'], test_size=0.2)\n",
    "\n",
    "print(\"Train data: \", len(tweets_train), \"\\nTest data: \", len(tweets_test),\n",
    "      \"\\nAll data: \", len(sentiment_train) + len(sentiment_test))\n",
    "\n",
    "\n",
    "\n",
    "# Let's recap the entire pipeline up to this point,\n",
    "# putting the steps explicitly into scikit-learn's Pipeline:\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=tokenizer.Stem)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Tune Parameters\n",
    "params = {\n",
    "#     'tfidf__use_idf': (True),\n",
    "    'bow__analyzer': (tokenizer.StemNoStops, tokenizer.LemmaNoStops,\n",
    "                      tokenizer.StemNoEnglishStops, tokenizer.LemmaNoEnglishStops,\n",
    "                      tokenizer.StemNoSoccerStops, tokenizer.LemmaNoSoccerStops,\n",
    "                      tokenizer.StemNoNegation, tokenizer.LemmaNoNegation,\n",
    "                      tokenizer.Stem, tokenizer.Lemma),\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,  # pipeline from above\n",
    "    params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(sentiment_train, n_folds=10),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's recap the entire pipeline up to this point,\n",
    "# putting the steps explicitly into scikit-learn's Pipeline:\n",
    "pipelineLog = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=tokenizer.Stem)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Tune Parameters\n",
    "params = {\n",
    "#     'tfidf__use_idf': (True),\n",
    "    'bow__analyzer': (tokenizer.StemNoStops, tokenizer.LemmaNoStops,\n",
    "                      tokenizer.StemNoEnglishStops, tokenizer.LemmaNoEnglishStops,\n",
    "                      tokenizer.StemNoSoccerStops, tokenizer.LemmaNoSoccerStops,\n",
    "                      tokenizer.StemNoNegation, tokenizer.LemmaNoNegation,\n",
    "                      tokenizer.Stem, tokenizer.Lemma),\n",
    "}\n",
    "\n",
    "gridLog = GridSearchCV(\n",
    "    pipelineLog,  # pipeline from above\n",
    "    params,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(sentiment_train, n_folds=10),  # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "%time nb_detector = grid.fit(tweets_train, sentiment_train)\n",
    "pprint(nb_detector.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_detector.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "%time log_detector = gridLog.fit(tweets_train, sentiment_train)\n",
    "pprint(log_detector.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_detector.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 3: Save Detecter\n",
    "\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('nb_tweets_pn_detector.pkl', 'wb') as fout:\n",
    "    pickle.dump(nb_detector, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
