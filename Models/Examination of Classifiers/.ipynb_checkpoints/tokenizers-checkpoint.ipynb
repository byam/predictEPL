{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from pprint import pprint\n",
    "from textblob import TextBlob\n",
    "\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/config/\")\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/utils/\")\n",
    "sys.path.append(\"/Users/Bya/git/predictEPL/WebScrapping/\")\n",
    "\n",
    "# configs\n",
    "import paths\n",
    "import names\n",
    "import espn_urls\n",
    "import soccer_stopwords\n",
    "import english_stopwords\n",
    "\n",
    "# utils\n",
    "import tokenizer\n",
    "import useful_methods\n",
    "import my_plot\n",
    "import scrap_espn_gamecast\n",
    "import result_analyzer\n",
    "import emolex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TweetLemmaSoccerNeg(text, stops=True):\n",
    "    # Checking negation words: 'not good'\n",
    "    # if found add suffix: 'not good_NEG'\n",
    "    neglect_text = tokenizer.NegationMark(text)\n",
    "    if neglect_text is not None:\n",
    "        text = neglect_text\n",
    "\n",
    "    # Text to Words\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "\n",
    "    # Lemma\n",
    "    words_lemma = []\n",
    "    for word in words:\n",
    "        # if word has negation mark: bad_neg, good_neg\n",
    "        if word.endswith('_neg'):\n",
    "            word = word[0:-4]\n",
    "            word = TextBlob(word).words[0].lemma + \"_neg\"\n",
    "\n",
    "        # regular words\n",
    "        else:\n",
    "            word = word.lemma\n",
    "\n",
    "        words_lemma.append(word)\n",
    "\n",
    "    if stops:\n",
    "        # Removing STOP WORDS(includes Soccer stops)\n",
    "        english_stops = set(stopwords.words('english'))\n",
    "        english_stops_soccer = english_stops | soccer_stopwords.STOP_WORDS\n",
    "\n",
    "        words_lemma = [word for word in words_lemma if word not in english_stops_soccer]\n",
    "\n",
    "    # Removing Twitter Links\n",
    "    words_lemma = [word for word in words_lemma if not word.startswith('t.co')]\n",
    "\n",
    "    return words_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text to Words\n",
    "# Lemmatizing & Remove English Stopwords\n",
    "def Lemma(text, stops=True):\n",
    "    # text to words as list\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "\n",
    "    # Lemma\n",
    "    words = [word.lemma for word in words]\n",
    "\n",
    "    if stops:\n",
    "        # Removing English STOPWORDS\n",
    "        STOP_WORDS = english_stopwords.STOP_WORDS\n",
    "        words = [word for word in words if word not in STOP_WORDS]\n",
    "\n",
    "    # Removing Twitter Links\n",
    "    words = [word for word in words if not word.startswith('t.co')]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text to Words\n",
    "# Lemmatizing & Remove English Stopwords\n",
    "def LemmaSoccer(text, stops=True):\n",
    "    # text to words as list\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "\n",
    "    # Lemma\n",
    "    words = [word.lemma for word in words]\n",
    "\n",
    "    if stops:\n",
    "        # Removing English STOPWORDS\n",
    "        words = [word for word in words if word not in STOP_WORDS]\n",
    "\n",
    "    # Removing Twitter Links\n",
    "    words = [word for word in words if not word.startswith('t.co')]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week = '13'\n",
    "team_home = 'City'\n",
    "team_away = 'Liverpool'\n",
    "\n",
    "dfSingle = useful_methods.SingleGameDf(week, team_home, team_away, filtering=True, retweet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just drafted Raheem Sterling &amp; Philippe Coutinho on @UltimateFanLive for #mcfc v #lfc  https://t.co/UFe3Cj6PA3 https://t.co/bInju4RYxS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['drafted',\n",
       " 'raheem',\n",
       " 'sterling',\n",
       " 'amp',\n",
       " 'philippe',\n",
       " 'coutinho',\n",
       " 'ultimatefanlive',\n",
       " 'mcfc',\n",
       " 'v',\n",
       " 'lfc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfSingle.text[0])\n",
    "Lemma(dfSingle.text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
