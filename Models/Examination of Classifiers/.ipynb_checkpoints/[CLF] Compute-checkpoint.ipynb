{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78c11f8c2ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../config/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0museful_methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_datas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tokenizer'"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Global Imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Local Imports\n",
    "sys.path.append(\"../utils/\")\n",
    "sys.path.append(\"../config/\")\n",
    "\n",
    "import tokenizer\n",
    "import useful_methods\n",
    "import train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# Data Load\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Data => Soccer Hash Emolex\n",
    "dic_emolex_soccer, y = emolex.EmolexSoccerDic()\n",
    "\n",
    "# Read Hash Emolex CSV\n",
    "dfHashEmolex = train_datas.HashEmolexAllRead()\n",
    "\n",
    "# Labeling Emolex 8 cat => POS, NEG\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(dfHashEmolex)):\n",
    "    text = dfHashEmolex.iloc[i]['text']\n",
    "    sentiment = dfHashEmolex.iloc[i]['sentiments'].split(',')\n",
    "    \n",
    "    for sent in sentiment:\n",
    "        if sent in ['anger', 'disgust', 'fear', 'sadness', 'negative']:\n",
    "            texts.append(text)\n",
    "            labels.append(0) # 0 is negative\n",
    "            break\n",
    "\n",
    "        elif sent in ['anticipation', 'joy', 'positive', 'trust']:\n",
    "            texts.append(text)\n",
    "            labels.append(1) # 1 is positive\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# Create New POS, NEG dataframe\n",
    "df = pd.DataFrame(columns=['text', 'label'])\n",
    "df['text'] = texts\n",
    "df['label'] = labels\n",
    "\n",
    "print(\"\\n\\nPOS: \", len(df[df.label == 1]))\n",
    "print(\"NEG: \", len(df[df.label == 0]))\n",
    "print(\"\\nAll: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_methods.DFtoCSV(\n",
    "    df=df,\n",
    "    pathToSave=\"/Users/Bya/Dropbox/Research/datas/TweetsPN/\",\n",
    "    fileName=\"tweet_hash_emolex_pn\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# Split data Train and Test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'],\n",
    "    df['label'],\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Train data: \\t\", len(X_train),\n",
    "    \"\\nTest data: \\t\", len(X_test),\n",
    "    \"\\nAll data: \\t\", len(y_train) + len(y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# [Naive Bayes]\n",
    "\n",
    "# putting the steps explicitly into Pipeline\n",
    "pipeline_nb = Pipeline([\n",
    "        # strings to token counts to weighted TF-IDF scores\n",
    "        ('vect', TfidfVectorizer(\n",
    "                analyzer=tokenizer.Lemma, # extract the sequence of features out of the raw\n",
    "                use_idf=True,             # Enable inverse-document-frequency reweighting\n",
    "                max_df=1.0,               # frequency threshold\n",
    "                max_features=None,        # max features\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        # train on vectors with classifier\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "\n",
    "# tunning parameters\n",
    "params_nb = {\n",
    "    'vect__analyzer': (\n",
    "        tokenizer.Stem,\n",
    "        tokenizer.Lemma\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# grid search\n",
    "grid_nb = GridSearchCV(\n",
    "    pipeline_nb,       # pipeline from above\n",
    "    params_nb,         # parameters to tune via cross validation\n",
    "    refit=True,         # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,          # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy', # what score are we optimizing?\n",
    "    cv=StratifiedKFold(y_train, n_folds=5), # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# [Logistic Regression]\n",
    "\n",
    "# putting the steps explicitly into Pipeline\n",
    "pipeline_log = Pipeline([\n",
    "        # strings to token counts to weighted TF-IDF scores\n",
    "        ('vect', TfidfVectorizer(\n",
    "                analyzer=tokenizer.Lemma, # extract the sequence of features out of the raw\n",
    "                use_idf=True,             # Enable inverse-document-frequency reweighting\n",
    "                max_df=1.0,               # frequency threshold\n",
    "                max_features=None,        # max features\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        # train on vectors with classifier\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "\n",
    "# tunning parameters\n",
    "params_log = {\n",
    "    'vect__analyzer': (\n",
    "        tokenizer.Stem,\n",
    "        tokenizer.Lemma\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# grid search\n",
    "grid_log = GridSearchCV(\n",
    "    pipeline_log,       # pipeline from above\n",
    "    params_log,         # parameters to tune via cross validation\n",
    "    refit=True,         # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,          # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy', # what score are we optimizing?\n",
    "    cv=StratifiedKFold(y_train, n_folds=5), # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# [Decision Trees]\n",
    "\n",
    "# putting the steps explicitly into Pipeline\n",
    "pipeline_dt = Pipeline([\n",
    "        # strings to token counts to weighted TF-IDF scores\n",
    "        ('vect', TfidfVectorizer(\n",
    "                analyzer=tokenizer.Lemma, # extract the sequence of features out of the raw\n",
    "                use_idf=True,             # Enable inverse-document-frequency reweighting\n",
    "                max_df=1.0,               # frequency threshold\n",
    "                max_features=None,        # max features\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        # train on vectors with classifier\n",
    "        ('clf', RandomForestClassifier(\n",
    "                criterion='entropy'\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "# tunning parameters\n",
    "params_dt = {\n",
    "    'vect__analyzer': (\n",
    "        tokenizer.Stem,\n",
    "        tokenizer.Lemma\n",
    "    ),\n",
    "    \n",
    "    'clf__criterion': (\n",
    "        'entropy',\n",
    "        'gini'\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# grid search\n",
    "grid_dt = GridSearchCV(\n",
    "    pipeline_dt,       # pipeline from above\n",
    "    params_dt,         # parameters to tune via cross validation\n",
    "    refit=True,         # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,          # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy', # what score are we optimizing?\n",
    "    cv=StratifiedKFold(y_train, n_folds=5), # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# [Support Vector Machines]\n",
    "\n",
    "# putting the steps explicitly into Pipeline\n",
    "pipeline_svm = Pipeline([\n",
    "        # strings to token counts to weighted TF-IDF scores\n",
    "        ('vect', TfidfVectorizer(\n",
    "                analyzer=tokenizer.Lemma, # extract the sequence of features out of the raw\n",
    "                use_idf=True,             # Enable inverse-document-frequency reweighting\n",
    "                max_df=1.0,               # frequency threshold\n",
    "                max_features=None,        # max features\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        # train on vectors with classifier\n",
    "        ('clf', SVC(\n",
    "                kernel='linear',\n",
    "                C=1, # defualt\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "# tunning parameters\n",
    "params_svm = {\n",
    "    'vect__analyzer': (\n",
    "        tokenizer.Stem,\n",
    "        tokenizer.Lemma\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# grid search\n",
    "grid_svm = GridSearchCV(\n",
    "    pipeline_svm,       # pipeline from above\n",
    "    params_svm,         # parameters to tune via cross validation\n",
    "    refit=True,         # fit using all available data at the end, on the best found param combination\n",
    "    n_jobs=-1,          # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy', # what score are we optimizing?\n",
    "    cv=StratifiedKFold(y_train, n_folds=5), # what type of cross validation to use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "\n",
    "%time grid_nb.fit(X_train, y_train)\n",
    "\n",
    "# print params\n",
    "DetecterParams(grid_nb, title=\"NB\", all_tunes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "\n",
    "%time grid_log.fit(X_train, y_train)\n",
    "\n",
    "# print params\n",
    "DetecterParams(grid_log, title=\"Log\", all_tunes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "\n",
    "%time grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# print params\n",
    "DetecterParams(grid_dt, title=\"DT\", all_tunes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "\n",
    "%time grid_svm.fit(X_train, y_train)\n",
    "\n",
    "# print params\n",
    "DetecterParams(grid_svm, title=\"SVM\", all_tunes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compare Detecters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DetecterMetrics(X_test, y_test, grid_log, title=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PlotRocAuc(X_test, y_test, grid_log, title=\"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print Test Prediction\n",
    "def DetecterMetrics(features, labels, detecter, title=\"\"):\n",
    "    predictions = detecter.predict(features)\n",
    "    \n",
    "    print(\"[%s Results]: \\n\" % title)\n",
    "    print(classification_report(labels, predictions))\n",
    "    print('Accuracy: ', accuracy_score(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print Training Results\n",
    "def DetecterParams(detecter, title=\"\", all_tunes=False):\n",
    "    if all_tunes:\n",
    "        print(\"[All Params Results]:\\n\")\n",
    "        pprint(detecter.grid_scores_)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"[%s Detecter Params]: \\n\" % title)\n",
    "    print(\"Best Score: \", detecter.best_score_)\n",
    "    print(\"Best Params: \", detecter.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Receiver Operating Characteristic = ROC curve\n",
    "# Visualizes a classifier's performance\n",
    "# for all values of the discrimination threshold. \n",
    "# fall out: F = FP / (TN + FP)\n",
    "# AUC (area under the curve)\n",
    "def PlotRocAuc(features, labels, detecter, title=\"\"):\n",
    "    # predict features\n",
    "    predictions = detecter.predict_proba(features)\n",
    "    \n",
    "    # calculate Fall Out & Recall\n",
    "    false_positive_rate, recall, thresholds = roc_curve(\n",
    "        labels, predictions[:, 1])\n",
    "\n",
    "    # ROC AUC\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "\n",
    "    # Plot\n",
    "    plt.title('Receiver Operating Characteristic: ' + title)\n",
    "    plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Step 3: Save Detecter\n",
    "\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('log_tweets_pn_detector-20160114.pkl', 'wb') as fout:\n",
    "    pickle.dump(log_detector, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Detecter\n",
    "os.chdir(\"/Users/Bya/Dropbox/Research/datas/Detecter/\")\n",
    "with open('log_tweets_pn_detector-20160114.pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'utf-8'\n",
    "    log_detector_reloaded = u.load()\n",
    "    classifier = log_detector_reloaded\n",
    "    print(log_detector_reloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in range(10):\n",
    "    tweet = dfTwitter.tweet[index]\n",
    "    label = dfTwitter.sentiment[index]\n",
    "    print(\"\\n\\n================================\")\n",
    "    print(\"[Tweet]:\\n\", tweet)\n",
    "    print(\"[Sentiment]: \", label)\n",
    "    print(\"\\n[Classifier]:\")\n",
    "    print (classifier.predict_proba(tweet)[0], classifier.predict(tweet)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
